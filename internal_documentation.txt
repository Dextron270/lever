When editing this file, tw=80, possibly set wrap

                                                              goal_standalone.py

runtime/goal_standalone.py is a file instructing rpython how to compile Lever.

Lever builds upon a runtime written in RPython. It functions and behaves like
python code, but has been constrained such that it can be translated into native
executable.

RPython originates from the PyPy -project. There it is used to translate python
written python interpreter into native machine code. It is capable of generating
a JIT compiler for an interpreter while it translates. Various annotations in
RPython code controls how the JIT compiler operates.

The author of RPython code does not need to worry about maintaining rules for
garbage collection. Translator takes care of inserting the necessary guards and
safety handles for the collectors.

When it is translated, the lever runtime is first imported by python
interpreter. The interpreter turns it into function objects and the entry
function is passed down to RPython. RPython starts annotating and specializing
the code according to the rules described on their documentation.



                                                                         base.py

base is a module in Lever. Many files in the runtime participate to building
it. It forms a default environment for modules.

The base environment contains lot of functionality.
The following is being inserted into base in base.py:

    dict
    module
    exnihilo
    object
    list
    multimethod(arity:int)
    float
    int
    bool
    str
    null
    true
    false
    path
    property
    Uint8Array
    set
    exec(program:dict, module)
    class(methods:exnihilo, parent:interface, name:str)
    interface(object)
    iter(object)
    hash(object)
    repr(object)
    reversed(object)
    getitem(object, index, value)
    setitem(object, index)
    listattr(object)
    getattr(obj, name)
    setattr(obj, name, value)
    ord(string)
    chr(value)
    isinstance(obj, interface/interfaces)
    print(values...)
    and(a, b)
    or(a, b)
    len(obj) # small func for .length, slightly pointless.
    not(a)
    encode_utf8(a:str)
    decode_utf8(a:Uint8Array)
    time()
    getcwd()
    chdir(path)
    exit(status:int=0)
    range()
          start, stop, step
          start, stop
          stop,
    input(prompt)
    print_traceback(exception)

Additionally base.py adds errors, operators and vector arithmetic objects
into base.

Base module is described in builtin modules, and therefore can be imported.



                                                                          bon.py
                                                                 stdlib/binon.py

Originally called bon, binon is a binary object notation used by Lever. It is
simple to decode and can be extended with custom encodings specific to Lever.

Lever parsing tends to be heavy and take its time, so it is important that the
compiled bytecode can be stored as files and reused between programs. Lever is
using binon to cache bytecode.

It is desired that binon isn't Lever specific format. It's meant to evolve along
Lever to interesting usecases, but there's no desire to lock it down to Lever
after it matures.

Glancing through the runtime, there's also json reader present, which is similar
to binon in many aspects. So binon is sort of an oddball. Also the use of binon
is restricted to files that aren't likely used by other software for now. 

Binon uses network byte endian (big endian).

Binon doesn't define an identifying header, but it is being defined in units:
    T -- 8-bit unsigned integer tag
    8 -- 8-bit unsigned integer
    32 -- 32-bit unsigned integer
    long -- [ T] =0
            Variable-length quantity integer.
            0x40 bit in the first byte denotes sign of the integer. 
            If 0x80 bit is set, there's additional digit.
            If 0x80 bit is unset, it is the last digit.
            the value is multiplied with the sign to make encoding easy.
    double -- [ T] =1
              run-off-the-mill IEEE floating point number.
              8 bytes wide
    string -- [ T] =2
              [32] length of the string in bytes
              [..] the string represented in utf-8 encoding.
    list -- [ T] =3
            [32] how many items in the list
            items of the list encoded with tagged units.

    dict -- [ T] =4
            [32] how many pairs in the dict
            key,value pairs encoded with tagged units.
    bytes -- [ T] =5
             [32] how many bytes in the array.
                  encodes to Uint8Array
                  Currently only reason why used instead of json.
    boolean -- [ T] =6
               [ 8] If 0x00, false
                    If 0x01, true
                    Other values are interpreted as true.
    null -- [ T] =7

Binon object may consist of any tagged unit.

The format is exposed to whole runtime as a standard library. You can import it
and get access to following functions:

    read_file(path:Object)
    write_file(path:Object, data:Object)

Writer not protected or designed to handle cyclic data structures.



                                                                continuations.py

Mechanism to implement continuations are provided by RPython. Last time I
checked the STM-version of pypy had them disabled, so continuations prevent
testing of STM with lever.

RPython stacklets are the easiest way to crash or break things, so there's the
continuations.py wrapper that ensures the interface is used correct.

This abstraction also makes it simpler to implement greenlets, those are the
actual representation of continuations in Lever.

A continuation object is bit like an evil mirror. Execution flow that creates
one gets trapped inside the continuation and a new execution takes its place.

Execution flow may switch places between an another execution trapped into the
continuation. If execution ends, and it is not the original, it must return a
continuation. The execution in that continuation takes its place and the
continuation becomes empty.

This sinister sounding control flow construct allows ordinary control flow to
wait for a value.

The advantage is that we don't need to break otherwise synchronous control flow
into callbacks, and there's no need to implement duplicate forms of control
flow with promises, or need to abuse iterators and generators for what
they weren't intended for.

Continuations add some challenges to working with resources such as file
descriptors. This kind of feature dissonance seem to be a recurring theme
across whole Lever. 



                                                             evaluator/loader.py

Loader is one of the things in runtime where there's lot of happening. The name
is a bit of misnomer because the loader code also holds the interpreter for
Lever.

This file defines how closures behave and how programs are loaded.

This file also contains some code the translator requires to produce JIT
compiler.

Instruction set for the interpreter is compact because operators are implemented
as functions and methods. Only "or", "not" and "and" are implemented via
instructions.

This is not the place to mention what each instruction does, but it's worthwhile
to mention how the interpreter is constructed:

 - Code is loaded as compilation units that usually span a single source file.
 - The compilation unit consists of list of functions. These functions can
   create a closure from any other function they share an unit with.
 - When the unit is loaded, The whole thing is wrapped into Program, that when
   called creates a Frame and runs the very first function in it.

 - Closure has access to the Frame of the function where it's been created.
   It can access variables in that frame. When closure is called it creates a
   new frame that parents the frame it had.

 - The execution context holds a virtual register array, and a bundle of locals.
   This separation is intended to avoid lugging the registers that might happen
   when you pass child closures around.

 - The exceptions are handled by exceptions -list. When interpreter
   returns into a frame with an exception it goes through the exceptions -list
   and jumps into the first entry that is active on the location of the current
   program counter. The exception is stored in the register pointed out by the
   exception structure.

Since the loader defines how the bytecode object looks like, lets describe the
format here:

{
    version = 0
    sources = [ str ] # list of source files this unit was compiled from.
    constants = [ object ] # List of constants for this unit.
    functions = [
        {
            flags = int # These aren't actually described anywhere, but..
                        # 0x1 -bit means that the remaining arguments are
                          put into local[topc]
            argc = int # minimum number of arguments
            topc = int # maximum number of recognized arguments
            localc = int # how many local variables there are in the frame.
            regc = int # how many virtual registers in this frame.
            code = Uint8Array # network-byte-endian encoded array of u16
            sourcemap = Uint8Array # described at evaluator/sourcemaps.py
            exceptions = [[start, stop, label, vreg]]
        }
    ]
}

When the object is loaded, it also gets a path -object that is given to
TraceEntry -records produced by the compilation unit. The point of this path is
to locate the source file when used in combination with 'sources' and
'sourcemap'.

Related:

    exec(unit, module, path)  loads an object and calls it using this system.
                              To help implementing REPL, the last value in
                              the compilation unit is returned by this function.



                                                            evaluator/optable.py
                                                               stdlib/optable.py

Some people like to type stupid magic numbers everywhere and move them across
files. I prefer that stuff works even if nobody is in supporting it with a stick.

The evaluator/optable.py contains name, opcode and format for every instruction
in the bytecode that Lever uses.

enc/dec tables are filled from the tabular specification. Both are dictionaries
that come in following format:
    
    enc[opname] = [opcode, has_result, [pattern], variadic]
    dec[opcode] = [opname, has_result, [pattern], variadic]

Examples:

    enc["call"] = [0x40, True,  ['vreg', 'vreg'], 'vreg']
    enc["jump"] = [0x60, False, ['block'],          null]

Every compiler and evaluators refers to these tables directly.

Changing optable isn't an offense as long as you have a good reason to do so.
For example, that the opcodes are all weirdly spread is not a good reason. A
good reason would be that you need a new instruction to support some usecase.

If you introduce a new instruction, try not to choose a number that was used
by earlier instruction to avoid issues. The old instructions are commented out
and it should be written in which version they were discarded to help you avoid
choosing a recently used number. The rule can be broken when the instructions
are so old that they have been nearly forgotten.

Please keep the opcode numbers in order and try to group the instructions by
their context, prefer alphabetic order otherwise.

The intent here is to not cause much issues with old bytecode. If you change
behavior of an instruction, please change the opcode as well.



                                                         evaluator/sourcemaps.py

TODO: rewrite this section to make it consistent.

The sourcemapping seen here is copied from the javascript world.

Sourcemaps are stored in Uint8Arrays to make their encoding and decoding into
files very fast. The purpose of this map is to translate program counter [pc]
values into a path and a position inside a file.

Every sourcemap consists of variable length quantity -encoded values of unsigned
kind. There are 6 VLQ encoded values in every record.

The record contains following values:

    count -- How many PC values are spanned by this record
    file_id -- Index to the sources -list.
    col0, lno0, col1, lno1 -- Range inside the file.

Every compilation unit has been annotated with 'sources' -list. It points out
relative paths to the source files the unit was compiled from.

The Lever compiler only compiles one source file into one output. But I am
aware there are situations where single bytecode unit might originate from
multiple sources files.

To retrieve the actual origin of the file, the path in "source" should be
attached to the directory path where the bytecode was loaded from.

CallErrors are particularly nasty if you're not aware of which function is
getting the bad arguments. To allow CallErrors point out the function it
originates from, the sourcemap holds a zero-count record in the beginning, where
you can put something where the CallError should point towards.

The same "header" record can be later used to "go to the source" of a function.



                                                                         main.py

Main is the heart of Lever. Main is the entry point of lever. This file also
implements greenlets.

Lever process has a global state that holds execution context. The execution
context hold up some values the context needs to function.

    lever_path -- The directory where lever is operating in.
                  If LEVER_PATH variable is not set, the program assumes the
                  execution takes place in current directory, or in the
                  prefix path given during translation.

Note that LEVER_PATH variable can cause problems if it's inappropriately set.


When Lever starts up, it comes up with an event loop. The event loop isn't much
yet. For now you can wait with sleep -function. Also nothing prevents you from
suspending execution yourself and scheduling it in. :)

Behavior during startup is bit chaotic. If the program gives arguments, the
first argument is treated as a script to run. Otherwise it will run a script
"app/main.lc" inside lever_path.

However it goes, the script is brought up with a new module scope.

The greenlets are wound up with the event loop to make both much more useful, so
they are in the same file. These are built along the earlier continuation
structures, but each greenlet associates with an execution, making them easier
to reason about.

Greenlets mesh neatly with the remaining system. You can either schedule them or
jump into them directly. There are many neat usecases where this is useful.
Particularly it makes event streams easy to wait on.

When greenlet ends, it returns to their parent. The parent is one that created
the greenlet, but it could be switched in theory.

It should be also possible to throw an error via the greenlet, either in sync or
async, to cause it alter in behavior, but this useful feature hasn't been
introduced yet. The likely functions will be:

    greenlet.throw(exception) # sync
    schedule(greenlet.throw, exception) # async

There are three dissonance situations with greenlets you may face.

One dissonance happens when you have a continuation that holds a resource that
must be explicitly freed. A memory buffer or file handle. If greenlet is
suspended and lost before it finishes execution, it also leaks these memory
handles even if they were seemingly properly allocated & freed.

So if you have something that can accept a greenlet, it is most likely good idea
to send an exception that tells it that a resource it waited on was discarded.

Second dissonance happens when you have an action that should not pass control,
as in it has atomicity requirements. This kind of function might accidentally
pass control by calling something that reads or otherwise may context switch.
This would violate the operation of the program and cause it to crash or corrupt
its state.

Third dissonance is a dual situation to the second. It is when you assume that
certain state is fixed during reading, and then you switch changinging the state
you were reading, and another execution change the state you were in middle of
reading.

It may be necessary to introduce a simple event loop lock mechanism to prevent that
kind of problems. Also you can control access to objects by means of visibility.
Execution that does not have access to something can also neither write to it.
Third you can introduce lot of stability into your programs by determining that
certain objects are immutable after they've been created.



Functions provided into base -module in main.py:

    schedule(function/greenlet, args...)
        - schedules a function or greenlet to start/resume up inside the event
          loop. Execution continues without interruption.
    sleep(...)
          d:float - suspends the current greenlet for d seconds.
          d:float, func/greenlet - schedules a function or resumes a greenlet
                                   after given time.
    getcurrent() - gets the current greenlet.

    greenlet(args...) - creates a new greenlet which runs call with the given
                        args when waked up.

    greenlet.swich(args...) - switches to the given greenlet.
                            - if the greenlet is just starting up, the arguments
                              are catenated with the initials
                            - otherwise zero arguments give null
                              one argument is passed as it
                              many arguments are turned into a list to return
                              from .switch() of the resuming greenlet.



                                                            module_resolution.py

I remember Python module system maintainer saying that Python modules aren't as
nice as they could be because there's a notion of mutable global state in them,
and that people are occassionally doing nasty things to this state to reach
their means. It turned up when I was researching on module systems.

Module system in dynamically typed language faces few challenges. Problems are
related to the fact that modules should be cached such that they can be accessed
my multiple modules that share dependencies.

Simplest and often most common way to implement coding of programs while they
are live is to write a script that is reloaded when it is updated. For module
systems this means you should be able to reload modules, or otherwise be able to
control which modules are reloaded and when.

The concepts related to module resolution aren't entirely honed up yet in Lever,
but there's very promising development going related to the problems described
above.

Lever comes with a concept I call scoped modules. I mean modules practically
have scopes, exactly like how functions have scopes. You can freely choose that
scope when you create a module or inherit from an another scope!

Any lever module scope can cache any path or resource and they are cached by
their absolute paths to help live reloaders to do their job. Very few
programs prepare for the situation that their path may change while they're
running, am I right?

Every module scope comes with a default search directory where to look modules
from. Every module scope also has a parent scope which is searched if the
current scope doesn't resolve the resource.

Every scope may also contain a handle to the function to compile stale modules
in the scope. The compile_file is retrieved for lib/ -scope before the main
scope is created.

Every module cache entry contains the mtime of the file that was loaded, and a
path to the file. So live reloaders know which modules are stale and should be
reloaded too.

When lever boots up, it already has a builtin:/ scope. It derives the lib/ scope
from this builtin scope. Then the main scope is created that points to the
directory of the first script. 

Lever supports directory hierarchies in modules. If you have a directory and
that directory contains "init.lc" or "init.lc.cb", then it's a module you can
import.

When a module is created in this system, it receives some attributes:

    dir    -- directory of the module, makes it easy to relative-load things.
    name   -- name of this module.
    import -- a callable object this module uses for importing code.

Import holds copy to the directory of the module, so that it can first search
relative to the location of the current module.

Then it searches from the scopes in order until it finds the module or fails to
import it. The imported module is returned by the call.

%"import".scope and you can access the current module scope as an object. This
scope object has one useful function such as: reimport It should supposedly
reimport your module but it looks like it might be half broken.

You can iterate through the scope, as well as getitem from it.

Note the "import" function is conveniently accessible. So you can always import
from the scope of an another module. Same applies to the "dir".

This is not implemented yet, but "foo.bar" -convention in lever should trigger
import of foo/init.lc, then getattr(init, "import")("bar"). Otherwise if you
import it directly, it should skip the foo/init.lc entirely.



                                                                      pathobj.py

It was always total and utter pain in the ass to take care of the correct
obeyance of path conventions present on the system. On Lever I decided to say
"fuck you" and denote that every path is posix-formatted inside lever.

To the OS, lever would still present whichever wrecked path convention the OS
follows. But instead of \\cupcake\boilingeggs on one system and something else
on another, the user would see //cupcake/boilingeggs.

The file pathobj.py takes care of this translation.

Lever comes with a path() -object. This path can take posix and URL prefixes. In
a way I consider URLs to be posix formatted paths to a limit.

When you wrap a string into a path object, it is converted into prefix and
path sequence.

this file provides following entries:

    PosixPrefix(label="", is_absolute=false)
    PosixPrefix.is_absolute -- whether the path starts with "/" or not.
                               eg. is absolute path and not relative.
    PosixPrefix.label       -- the thing before colon, eg. "c:"
    URLPrefix(domain="", protocol="")
    URLPrefix.protocol  -- eg. "http"
    URLPrefix.domain    -- eg. "example.org"
    path.prefix -- either PosixPrefix or URLPrefix now.
    path.basename -- basename of the path.. eg. thing after last "/"
    path.relpath(base=getcwd()) -- turn the path into relative path in respect
                                   base. It may still stay absolute if base has
                                   completely different prefix.
    path.push(name) -- push a name into the path sequence.
    path.get_os_path(path) -- transmoglify the path into OS conventions.
    path.to_string() - return posix formatted path.

    str ++ path
    path ++ str
    path ++ path

Also implements getcwd, chdir, to_path, so on..



                                                               space/__init__.py
                                                                space/builtin.py
                                                           space/customobject.py
                                                             space/dictionary.py
                                                                 space/errors.py
                                                               space/exnihilo.py
                                                              space/interface.py
                                                             space/listobject.py
                                                                 space/module.py
                                                            space/multimethod.py
                                                                space/numbers.py
                                                              space/operators.py
                                                              space/setobject.py
                                                                 space/string.py
                                                             space/uint8array.py

                                                                   stdlib/api.py
                                                           stdlib/ffi/bitmask.py
                                                          stdlib/ffi/__init__.py
                                                            stdlib/ffi/simple.py
                                                           stdlib/ffi/systemv.py



                                                                    stdlib/fs.py

Every module that wants to read or write something into file is using this
module. This is quite rudimentary module without beauties.

I would hope to combine the filesystem module with eventloop at some point, and
have the methods in this library work asynchronously, returning control to the
internal event loop when process would have to sleep otherwise.

'fs' exposes following objects:
    
    exists(path)
    stat(path)
    getatime(path)
    getmtime(path)
    getctime(path)
    read_file(path, mode="")
    open(path, mode="")
    file
    file.read(count=null)
    file.write(data:Uint8Array)
    file.close()
    file.seek(pos:int)
    file.tell()
    file.truncate(pos:int)
    file.fileno()
    file.isatty()
    file.flush()



                                                                    stdlib/gc.py

I would hope to have some control over GC. But RPython methods that seemed
relevant didn't turn out to control anything.

All this 'gc' module exposes is:

    collect()

I don't know if it triggers a collector cycle.



                                                              stdlib/__init__.py

When I have a directory containing names, it would feel off that have to write
a list of modules present in Lever. A little script in the stdlib/__init__.py
did really brightened up my mood.



                                                                  stdlib/json.py

I'm certain I wouldn't definitely copy this JSON reader. I'm not sure it handles
every special case.

This reader was introduced to read the generated C header files that I encoded
as json for convenience. It's pretty cool how fast loading json files can be.

This is a rare case where hand-written parsers have their merits.

Json files are also used for configs. It's pretty clear so far that they rock
for lots of common uses.

Exposed function:

    read_file(path:Object)

It might be a neat weekend project to improve this module.



                                                              stdlib/platform.py

Sometimes it's really necessary to know what your platform looks like. Every
interface isn't portable, and everyone can make mistakes.

Exposed values:

    name  # Describes the name of the platform. win32 or linux.
            This is derived from the sys.platform directly.



                                                               stdlib/process.py

Consider that you want to communicate with other programs. Chances are you want
to spawn processes. This library was written to get blender export stale models
automatically.

I'm honestly not sure how the spawnv should work. The first argument variable
seem to be often unimportant. But I know how it would suck if some app depended
on specific argv[0] and you couldn't trigger it with this code.

The module 'process' contains following objects:

    spawnv(path, args:list)
    waitpid(pid:int)
    which(program:path)
    is_exe(file:path)



                                                                         util.py

Lot of things seem to fall into place in Lever, eventually. When they don't,
they end up to the util.py At 1.0.0 this file might be empty or not be there.



                                                                   vectormath.py

Interactive graphics, music.. VR... It seems inevitable that the runtime needs
to deal with vector arithmetic. It would also eventually have to make it very
fast.

Therefore we have some vector arithmetic, and it's defined and pushed into the
base module! The functionality here isn't complete, but it is already
sufficient. There are some OpenGL 4 demos in the samples directory.

Main reason why this code is in base module is that I were not sure how
"from x import ..." should function exactly. It, again, presents dissonance
case for reloadable modules. This is one method to leave stale marks from
earlier modules into the runtime.

Vectormath also holds some randomizing functions and trigonometry functions.
It's steering on the limit of containing gunk. Possibly some of these features
have to be separated into modules later.

vec3, quat, mat4 are iterable and all have .length

The vectormath.py introduce the following values into base module:

    vec3(x=0, y=0, z=0)
    quat(x=0, y=0, z=0, w=1)

    quat.to_mat4(position:vec3=vec3())
    quat.invert()
    axisangle(axis:vec3, angle:float)
    mat4(values...)
    mat4.transpose()
    mat4.invert()
    mat4.adjoint()
    mat4.determinant()
    mat4.rotate_vec3(vec3)
    mat4.translate(vec3)
    mat4.scale(vec3)
    random()
    random_circle()
    random_sphere()
    sin(float)
    cos(float)
    tan(float)
    asin(float)
    acos(float)
    atan(float)
    atan2(y, x)
    sqrt(float)
    projection_matrix(fovy_radians, aspect, znear, zfar)
    left
    right
    up
    down
    forward
    backward
    length(value)
    dot(a, b)
    cross(a, b)
    normalize(a)
    pi
    tau

Vectormath introduce the following functions as multimethods:

    vec3 + vec3
    vec3 - vec3
    vec3 * float
    float * vec3
    vec3 / float
    float / vec3
    length(vec3)
    dot(vec3, vec3)
    cross(vec3, vec3)
    normalize(vec3)
    -vec3
    -quat
    +vec3
    +quat
    quat * quat
    quat * vec3
    mat4 * vec3
    mat4 * mat4
    clamp(float, float, float)
    clamp(int, int, int)
