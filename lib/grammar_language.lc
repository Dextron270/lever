import chartparser
import tokenizer

# Serves as an example of use. Once you do this, you may proceed to
# use the grammar just like how this module uses its own parsing table.
main = ():
    language = read_file("lever-0.8.0.grammar")
    language.new_parser = chartparser.preprocess(
        language.grammar,
        language.table.nonterminal("file"))
    for rule in language.grammar
        print(rule)
    # You can prefix a string to use it as a symbol.
    # Therefore using scoped objects to build action table
    # will not require workarounds in notating the grammar.
    actions = :exnihilo()
        %"return" = 123

read_file = (filename, source=filename):
    tokens = tokenizer.read_file(filename, language.table.keywords)
    return read_tokens(tokens, source)

read_string = (string, source=null):
    tokens = tokenizer.read_string(string, language.table.keywords)
    return read_tokens(tokens, source)

read_tokens = (tokens, source=null):
    parser = language.new_parser()
    indent = :chartparser.IndentParser(tokens[0].start)
        indent = language.table.terminal("indent")
        dedent = language.table.terminal("dedent")
        newline = language.table.terminal("newline")
    for token in tokens
        indent.step(parser, token.start, source)
        terminal = language.table.terminal(token.name)
        if not parser.expecting(terminal)
            raise chartparser.SyntaxErrorExpected(parser.expect, token.start, source)
        parser.step(terminal, token, token.start, token.stop)
    indent.finish(parser, token.stop)
    if not parser.accepted
        raise chartparser.SyntaxErrorExpected(parser.expect, token.stop, source, true)
    table = SymbolTable()
    # Some of the nonterminals are speudonyms to terminals. 
    # Except that it's a bad idea.
    #for t in ["indent", "dedent", "newline", "string", "symbol", "int", "hex", "float"]
    #    table.nonterminals[t] = table.terminal(t)
    post = invoker(actions, table)
    blank_lhs = (x): # Not used in this grammar.
        return null
    return :exnihilo()
        table = table
        grammar = parser.traverse(post, blank_lhs)

invoker = (actions, argl...):
    return (rule, args, start, stop):
        return rule.annotation(actions, args, argl)

build_language = ():
    table = SymbolTable()
    Rule = chartparser.Rule

    file = table.nonterminal("file")
    rule = table.nonterminal("rule")
    production = table.nonterminal("production")
    productions = table.nonterminal("productions")
    inline_productions = table.nonterminal("inline_productions")
    term = table.nonterminal("term")
    block = table.nonterminal("block")

    indent = table.terminal("indent")
    dedent = table.terminal("dedent")
    newline = table.terminal("newline")
    string = table.terminal("string")
    symbol = table.terminal("symbol")

    arrow = table.keyword("=>")
    comma = table.keyword(",")
    lc = table.keyword("{")
    rc = table.keyword("}")
    lb = table.keyword("[")
    rb = table.keyword("]")
    at = table.keyword("@")
    skip = table.keyword("%")
    grab = table.keyword("^")

    # There will be lot of things asked, such as. Why pick @ as a symbol to
    # name classes of tokens?
    # '=' would be confused to arrow =>
    # Symbols such as +, -, ?, * didn't go because they appear in regexes.
    # '#' was a good contender.. might work as well. I liked the round char here
    #                            for some reason.
    # ':' was not picked because we are going to use it for argument ordering
    # like this: op { 1:lhs 0:op 2:rhs }
    # There would have been easy confusion when someone forgets a colon.

    # The terminals needed to be identified in grammars though.
    # Mixing up nonterminals and terminals would easily lead to grammar
    # bugs. And this makes grammars bit more generic so that nobody doesn't
    # need to pass a list of understood tokens to the grammar.

    grammar = [
        Rule(file, [rule],
            Grab(0)),
        Rule(file, [file, newline, rule],
            Command("extend", [Grab(0), Grab(2)])),
        Rule(rule, [symbol, arrow, production],
            Command("rule", [Grab(0), Command("first", [Grab(2)])])),
        Rule(rule, [symbol, arrow, block],
            Command("rule", [Grab(0), Grab(2)])),
        Rule(rule, [symbol, arrow, lb, inline_productions, rb],
            Command("rule", [Grab(0), Grab(3)])),
        Rule(block, [indent, productions, dedent],
            Grab(1)),
        Rule(productions, [lb, inline_productions, rb],
            Grab(1)),
        Rule(productions, [production],
            Command("first", [Grab(0)])),
        Rule(productions, [productions, newline, production],
            Command("append", [Grab(0), Grab(2)])),
        Rule(production, [production, term],
            Command("append", [Grab(0), Grab(1)])),
        Rule(production, [term],
            Command("first", [Grab(0)])),
        Rule(production, [lc, production, rc],
            Grab(1)),
        Rule(production, [lc, rc],
            Command("empty", [])),
        Rule(inline_productions, [production],
            Command("first", [Command("force_grab", [Grab(0)])])),
        Rule(inline_productions, [inline_productions, comma, production],
            Command("append", [Grab(0), Command("force_grab", [Grab(2)])])),
        Rule(term, [skip, at, symbol],
            Command("skip", [Command("terminal", [Grab(2)])])),
        Rule(term, [grab, at, symbol],
            Command("grab", [Command("terminal", [Grab(2)])])),
        Rule(term, [skip, symbol],
            Command("skip", [Command("symbol", [Grab(1)])])),
        Rule(term, [grab, symbol],
            Command("grab", [Command("symbol", [Grab(1)])])),
        Rule(term, [skip, string],
            Command("skip", [Command("string", [Grab(1)])])),
        Rule(term, [grab, string],
            Command("grab", [Command("string", [Grab(1)])])),
        Rule(term, [string],
            Command("skip", [Command("string", [Grab(0)])])),
        Rule(term, [symbol],
            Command("grab", [Command("symbol", [Grab(0)])])),
        Rule(term, [at, symbol],
            Command("grab", [Command("terminal", [Grab(1)])])),
        Rule(term, [symbol, lc, rc],
            Command("command", [Grab(0)])),
        Rule(term, [symbol, lc, production, rc],
            Command("command", [Grab(0), Grab(2)])),
    ]
    return :exnihilo()
        grammar = grammar
        table = table
        new_parser = chartparser.preprocess(grammar, file)

actions = :exnihilo()
    command = (table, symbol, arg=[]):
        return :exnihilo()
            name = "command"
            command = symbol.string
            arg = arg
    first = (table, item):
        return [item]
    append = (table, items, item):
        items.append(item)
        return items
    extend = (table, items, item):
        items.extend(item)
        return items
    skip = (table, arg):
        return :exnihilo()
            name = "skip"
            arg = arg
    grab = (table, arg):
        return :exnihilo()
            name = "grab"
            arg = arg
    terminal = (table, token):
        return table.terminal(token.string)
    symbol = (table, token):
        return table.nonterminal(token.string)
    string = (table, token):
        return table.keyword(token.string)
    rule = (table, symbol, productions):
        lhs = table.nonterminal(symbol.string)
        result = []
        for production in productions
            result.append(make_rule(lhs, production))
        return result
    force_grab = (table, productions):
        for cell in productions
            if cell.name == "skip"
                cell.name = "grab"
        return productions

make_rule = (lhs, production):
    rhs = []
    annotation = make_command(rhs, null, production)
    return chartparser.Rule(lhs, rhs, annotation)

make_command = (rhs, command, production):
    clist = []
    for prod in production
        if prod.name == 'skip'
            rhs.append(prod.arg)
        elif prod.name == 'grab'
            clist.append(Grab(rhs.length))
            rhs.append(prod.arg)
        elif prod.name == 'command'
            clist.append(make_command(rhs, prod.command, prod.arg))
        else
            raise Error(repr(prod.name))
    if command
        return Command(command, clist)
    elif clist.length == 0
        return blank
    elif clist.length == 1
        return clist[0]
    else
        return Group(clist)

class Grab
    +init = (self, index):
        self.index = index

    +call = (self, actions, args, argl):
        return args[self.index]

class Command
    +init = (self, name, args):
        self.name = name
        self.args = args

    +call = (self, actions, args, argl):
        action = getattr(actions, self.name)
        argv = list(argl)
        for arg in self.args
            argv.append(arg(actions, args, argl))
        return action(argv...)

blank = (actions, args, argl):
    return null

class Group
    +init = (self, args):
        self.args = args

    +call = (self, actions, args, argl):
        group = []
        for arg in self.args
            group.append(arg(actions, args, argl))
        return group

class SymbolTable
    +init = (self, terminals={}, nonterminals={}, keywords={}):
        self.terminals = terminals
        self.nonterminals = nonterminals
        self.keywords = keywords

    terminal = (self, name):
        try
            return self.terminals[name]
        except KeyError as _
            self.terminals[name] = s = chartparser.Terminal(name)
            return s

    nonterminal = (self, name):
        try
            return self.nonterminals[name]
        except KeyError as _
            self.nonterminals[name] = s = chartparser.Nonterminal(name)
            return s

    #TODO: remove
    #symbol = (self, name):
    #   self.symbols[name] = t = self.terminal(name)
    #   return t

    keyword = (self, name, smear=default_smear):
        # Parsing would break down if some of speudonymous terminals were used as
        # keywords in a language. Most of them aren't very "sexy" keywordy names, so
        # I wouldn't expect them to be used. But...
        # Prefixing keywords will prevent the problem from ever appearing.
        # Besides it makes the repr(Rule(...)) tiny little bit more readable.
        if name not in self.keywords
            for keyword in smear(name)
                self.keywords[keyword] = "'" ++ keyword ++ "'"
        return self.terminal("'" ++ name ++ "'")

# The tokenizer we use requires symbol-keywords to be
# smeared in order for them to be recognized.
# But if we do the same for textual keywords, it will result in a mess.
default_smear = (keyword):
    for ch in keyword
        if ch.is_alpha()
            return [keyword]
    result = []
    prefix = []
    for ch in keyword
        prefix.append(ch)
        result.append("".join(prefix))
    return result

language = build_language()
