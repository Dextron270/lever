# This library implements C tokenizing and parsing.
# If you need to read C headers or parse C code, this should fit the bill.

# It combines two of my earlier python projects: cffi-gen and pytci
import chartparser, grammar_language
import fs, json

doc = runtime_path ++ "doc/c"

# Note this program doesn't test for the grammar. 
main = ():
    stdio_h = "/usr/include/stdio.h"
    macro_replacements = std_macro_env()
    # This is actually something exposed to user. The user can select a config
    # The config comes from pytci's gcc_probe
    # Ran it with gcc_probe.py gcc -m32
    # and without the -m32
    config = json.read_file("/home/cheery/.local/cc-config-x86_64.json")

    # giving the preprocessor a stream allows you to stream
    # from elsewhere than files. Although this uses the include like usual.
    preprocessor = Preprocessor(config, macro_replacements, open_token_stream(stdio_h))

    # Printing tokens out is easy way to see that the stream is correctly tokenized.
    filename = preprocessor.filename
    lno = 1
    unprinted = []
    while not preprocessor.empty
        token = preprocessor.step()
        if not token # preprocessor returns 'null' also 
            continue # when macro processing is happening.

        # This thing here is trying to imitate the weird line switching logic
        # of gcc token printer. I think I get it wrong. But then I'm not yet
        # handling macros either.
        if token.lno != lno or token.filename != filename
            if token.filename == filename
                print(" ".join(unprinted))
                if lno < token.lno and token.lno < lno + 6
                    for i in range(lno, token.lno-1)
                        print("")
                else
                    print("")
                    print("#", token.lno, escape_string(filename.to_string()))
            else
                print("")
                print("#", token.lno, escape_string(filename.to_string()))
            filename = token.filename
            lno = token.lno
            unprinted = []
        unprinted.append(stringify_token(token.name, token.value))
    if unprinted.length > 0
        print(" ".join(unprinted))

# The semantic labels of ansi_c grammar and the macroeval grammar differ
# for no reason. If they need to be unified later, then the macroeval file
# will receive changes.
language = grammar_language.read_file(dir ++ "ansi-c.grammar")
language.new_parser = chartparser.preprocess(
    language.grammar,
    language.table.nonterminal("external_declaration"))

parse_all = (preprocessor, scope):
    results = []
    res = parse(preprocessor, scope)
    while res
        results.append(res)
        res = parse(preprocessor, scope)
    return results

parse = (preprocessor, scope):
    parser = language.new_parser()
    parser.default_ambiguity_resolution = ambiguity
    keywords = language.table.keywords
    terminals = language.table.terminals
    while not preprocessor.empty
        token = preprocessor.step()
        if not token
            continue
        if token.name == "identifier" and token.value in keywords
            term = terminals.get(keywords.get(token.value))
# ... interesting...... We don't use this, actually.
#        elif token.name == "identifier" and token.value in typetable
#            term = terminals["type_name"]
        else
            term = terminals.get(keywords.get(token.name, token.name))
        assert parser.expecting(term)
            print("EXPECTED", parser.expect...)
            print(format_source(token), "GOT", term, token)
            "C parsing failed" #TODO: improve these error messages.
        parser.step(term, token, token, token)
        if parser.accepted
            result = parser.traverse(post)
            result = result(scope)
            assert result or preprocessor.empty
                "bug in parser, parse() shouldn't return null until end."
            return result
    assert parser.chart.length == 1, "parser not finished?"


# Yes, this is hairy the spider.
# You'll be fine as long as you don't touch it.
# The hairy takes care of ambiguity in C grammar.

# Note that hairy doesn't corrupt scope, because declaration
# is no longer ambiguous when the declaration() gets called.

# Note that hairy neither messes up with list/first/append
# because there is no ambiguity in C list rules.
# well.. with the grammar we use..
ambiguity = (sppf, traverse):
# was used for diagnosis
#    print("ambiguity detected on", sppf.cell, format_source(sppf.start))
#    print(sppf.start, sppf.stop)
#    for branch in sppf
#        print("BRANCH")
#        for sub in branch
#            print("  ", sub.cell)
#            print("    ", sub.start, sub.stop)
    fn = (scope, args...):
        res = []
        for branch in sppf
            try
                res.append(traverse(branch)(scope, args...))
            except Conflict as conflict
                null
        # Should perhaps coalesce the conflicts if we get to raise.
        if res.length == 0 # A merit in storing traceback on your exception.
            raise conflict # This retains the traceback.
        assert res.length == 1
            print(format_source(sppf.start), sppf.start)
            print(format_source(sppf.stop), sppf.stop)
            "unable to resolve ambiguity" # should never happen in C.
        return res[0]
    return chartparser.Resolve(fn)

class Conflict extends Exception
    +init = (self, token, which):
        self.traceback = null
        self.token = token
        self.which = which

    +repr = (self):
        return format_source(self.token) ++ "Conflict on " ++ repr(self.token) ++
            ", it was expected to be " ++ self.which

class Scope
    +init = (self, parent=null):
        self.parent = parent
        self.variables = {}
        self.tags = [] # structures, enums, unions within this scope.
        self.decls = [] # all declarations.
        self.defs = [] # all definitions.

    get = (self, name):
        if name in self.variables
            return self.variables[name]
        if self.parent
            return self.parent.get(name)
        return null

    is_type = (self, name):
        try
            info = self.variables[name]
            return info.type == "type"
        except KeyError as _
            return false

    declare = (self, name, type, declarator):
        self.variables[name] = decl = :exnihilo()
            type = type
            declarator = declarator
        self.decls.append(decl)

    declare_enum = (self, name, enum):
        self.variables[name] = decl = :exnihilo()
            type = "enum"
            enum = enum

post = (rule, args, start, stop):
    res = rule.annotation(actions, args, [])
    # Relatively simple hack that lets us locate where
    # each cell comes from.
    if isinstance(res, interface(post))
        return (scope, args...):
            res = res(scope, args...)
            if isinstance(res, Node)
                res.origin = start
            return res
    return res

actions = :exnihilo()
    function = (specifier, declarator, functypes, body):
        return (scope):
            spec = specifier(scope)
            declarator = declarator(scope)
            declarator.specifier = spec
            # Old style function definitions described argument types
            # below declaration. This style seems cleaner in some places,
            # although its use has been discouraged.
            ft = []
            for decl in functypes
                ft.append(decl(scope))
            declarator.oldtypes = ft
            # Maybe they think that it's easy enough to extend the
            # declarator to multiple lines, so that's why it's
            # deprecated.
            func = Function(declarator, body(scope))
            scope.defs.append(func)
            return func

    declaration = (specifier, declarator):
        return (scope):
            spec_or_decl = specifier(scope)
            declarator = declarator(scope)
            declarator.specifier = spec_or_decl.specifier
            if spec_or_decl != spec_or_decl.specifier
                declarator.last = spec_or_decl
            if declarator.specifier.storage == "typedef"
                type = "type"
            else
                type = "variable"
            scope.declare(declarator.name, type, declarator)
            return declarator

    initializer = (declarator, const):
        return (scope):
            decl = declarator(scope)
            decl.initializer = const(scope)
            return decl
    array_initializer = (seq):
        return (scope):
            i = []
            for x in seq
                i.append(x(scope))
            return i

    declarator = (identifier=null):
        return (scope):
            return Declarator(identifier)
    bitfield = (declarator, constant):
        return (scope):
            return :exnihilo()
                type = "bitfield"
                declarator = declarator(scope)
                count = constant(scope)

    pointer_declarator = (ptr, base):
        return (scope):
            declarator = base(scope)
            declarator.stack = :exnihilo()
                next = declarator.stack
                type = "pointer"
                ptr = ptr
            return declarator
    array_declarator = (base, const):
        return (scope):
            declarator = base(scope)
            declarator.stack = :exnihilo()
                next = declarator.stack
                type = "array"
                if const
                    size = const(scope)
                else
                    size = null
            return declarator
    function_declarator = (base, arglist):
        return (scope):
            declarator = base(scope)
            # The arglists contain spiders.
            args = []
            for arg in arglist
                args.append(arg(scope)) # and spiders want out.
            declarator.stack = :exnihilo()
                next = declarator.stack
                type = "function"
                args = args 
            return declarator
    parameter = (specifier, declarator):
        return (scope):
            spec_or_decl = specifier(scope)
            declarator = declarator(scope)
            declarator.specifier = spec_or_decl.specifier
            # This 'declarator' is also a declarator.
            # Note that even if parameter list is filled with
            # declarators, it may also contain specifiers.
            return declarator
    va_args = (parameters):
        fn = (scope):
            return "..."
        parameters.append(fn)
        return parameters

    specifier = ():
        return (scope):
            return Specifier()
    set_storage_class_specifier = (specifier, storage):
        return (scope):
            specifier = specifier(scope)
            # Multiple storage classes is an error.
            if specifier.storage
                raise Conflict(storage, "already has one")
            specifier.storage = storage.value
            return specifier
    add_type_specifier = (specifier, type):
        return (scope):
            specifier = specifier(scope)
            specifier.typespec.append(type(scope))
            return specifier
    add_type_qualifier = (specifier, qualifier):
        return (scope):
            specifier = specifier(scope)
            specifier.qualifiers.add(qualifier.value)
            return specifier

    primitive_type = (identifier):
        return (scope):
            return identifier.value

    as_type = (identifier):
        return (scope):
            if not scope.is_type(identifier.value)
                raise Conflict(identifier, "type")
            return Reference(identifier.value, scope.get(identifier.value))

    as_name = (identifier):
        return (scope):
            if scope.is_type(identifier.value)
                raise Conflict(identifier, "variable")
            return Reference(identifier.value, scope.get(identifier.value))

    data_specifier = (which, name, fields):
        return (scope):
            flds = []
            ds = DataSpecifier(which, name, [])
            for field in fields
                field(scope, ds)
            scope.tags.append(ds)
            return ds
    struct_declaration = (specifier, declarator):
        return (scope, ds):
            spec_or_decl = specifier(scope, ds)
            declarator = declarator(scope)
            declarator.specifier = spec_or_decl.specifier
            if spec_or_decl != spec_or_decl.specifier
                declarator.last = spec_or_decl
            ds.fields.append(declarator)
            return declarator

    enum_specifier = (name, enums):
        return (scope):
            enum = EnumSpecifier(name)
            for f in enums
                f(scope, enum)
            scope.tags.append(enum)
            return enum
    enumerator = (name, const):
        return (scope, enum):
            # Smearing of the enum is left to the application.
            # This is done because there is no certainty what the
            # user wants to do yet.
            if const
                c = const(scope)
            else
                c = null
            scope.declare_enum(name.value, enum)
            enum.enums.append([name, c])

    compound = (seq):
        return (scope):
            scope = Scope(scope)
            body = []
            for x in seq
                body.append(x(scope))
            return Compound(scope, body)

    label = (name, stmt):
        return (scope):
            return Label(name, stmt(scope))

    case_label = (name, stmt):
        return (scope):
            return Case(name, stmt(scope))

    cond = (cond, body, othw=null):
        return (scope):
            cond = cond(scope)
            body = body(scope)
            if othw
                othw = othw(scope)
            return Cond(cond, body, othw)
    inline_cond = (cond, body, othw):
        return (scope):
            return Cond(cond(scope), body(scope), othw(scope))
            

    switch = (expr, stmt):
        return (scope):
            return Switch(expr(scope), stmt(scope))

    %"while" = (expr, stmt):
        return (scope):
            return While(expr(scope), stmt(scope))
    do_while = (stmt, expr):
        return (scope):
            return DoWhile(stmt(scope), expr(scope))
    %"for" = (start, stop, step, stmt):
        return (scope):
            return For(
                start(scope), stop(scope), step(scope),
                stmt(scope))

    goto = (where):
        return (scope):
            return Goto(where)
    %"break" = ():
        return (scope):
            return Break()
    %"continue" = ():
        return (scope):
            return Continue()
    %"return" = (what):
        return (scope):
            return Return(what(scope))

    cast = (tp, expr):
        return (scope):
            return Cast(tp(scope), expr(scope))
    call = (callee, args):
        return (scope):
            callee = callee(scope)
            argv = []
            for arg in args
                argv.append(arg(scope))
            return Call(callee, args) 
    getitem = (base, index):
        return (scope):
            return GetItem(base(scope), index(scope))
    dot = (expr, name):
        return (scope):
            return GetAttr(expr(scope), name, false)
    arrow = (expr, name):
        return (scope):
            return GetAttr(expr(scope), name, true)

    assign = (lhs, op, rhs):
        return (scope):
            return Assign(lhs(scope), op, rhs(scope))
    prefix = (op, rhs):
        return (scope):
            return Prefix(op, rhs(scope))
    postfix = (lhs, op):
        return (scope):
            return Postfix(lhs(scope), op)
    binary = (lhs, op, rhs):
        return (scope):
            return Binary(lhs(scope), op, rhs(scope))
    sizeof = (spec):
        return (scope):
            return Sizeof(scope, spec(scope))
    const = (value):
        return (scope):
            return Constant(value)
    string_const = (seq):
        return (scope):
            return StringConstant(value)
    nop = ():
        return (scope):
            return null

    nil = ():
        return null
    list = (args...):
        return args
    first = (a):
        return [a]
    append = (a, b):
        a.append(b)
        return a

class Node
    null

# Parser transforms its input into wide variety of objects.
class Function extends Node
    +init = (self, declarator, body):
        self.origin = null
        self.declarator = declarator
        self.body = body

class Declarator extends Node
    +init = (self, token):
        self.origin = null
        self.token = token
        if token
            self.name = token.value
        else
            self.name = null
        self.specifier = null
        self.stack = null
        self.last = null
        self.initializer = null

    +repr = (self):
        if self.name
            return "Declarator{" ++ repr(self.specifier) ++ ", " ++ self.name ++ "}"
        return "Declarator{" ++ repr(self.specifier) ++ "}"

class Specifier extends Node
    +init = (self):
        self.origin = null
        self.storage = null
        self.typespec = []
        self.qualifiers = set()
        self.specifier = self

    +repr = (self):
        out = []
        if self.storage
            out.append(self.storage)
        for spec in self.typespec
            out.append(repr(spec))
        out.extend(self.qualifiers)
        return "Specifier{" ++ " ".join(out) ++ "}"

class DataSpecifier extends Node
    +init = (self, token, name, fields):
        self.origin = null
        self.token = token
        self.name = name
        self.fields = fields

class EnumSpecifier extends Node
    +init = (self, name):
        self.origin = null
        self.name = name
        self.enums = []

class Assign extends Node
    +init = (self, lhs, op, rhs):
        self.origin = null
        self.lhs = lhs
        self.op = op
        self.rhs = rhs

class Binary extends Node
    +init = (self, lhs, op, rhs):
        self.origin = null
        self.lhs = lhs
        self.op = op
        self.rhs = rhs

class Prefix extends Node
    +init = (self, op, rhs):
        self.origin = null
        self.op = op
        self.rhs = rhs

class Postfix extends Node
    +init = (self, lhs, op):
        self.origin = null
        self.lhs = lhs
        self.op = op

class Sizeof extends Node
    +init = (self, sz):
        self.origin = null
        self.sz = sz

class Constant extends Node
    +init = (self, token):
        self.origin = null
        self.token = token

class StringConstant extends Node
    +init = (self, tokens):
        self.origin = null
        self.tokens = tokens

class Reference extends Node
    +init = (self, name, value):
        self.origin = null
        self.name = name
        self.value = value

    +repr = (self):
        return "Ref{" ++ self.name ++ "}"

class Compound extends Node
    +init = (self, scope, body):
        self.origin = null
        self.scope = scope
        self.body = body

class Label extends Node
    +init = (self, name, stmt):
        self.origin = null
        self.name = name
        self.stmt = stmt

class Case extends Node
    +init = (self, name, stmt):
        self.origin = null
        self.name = name
        self.stmt = stmt

class Cond extends Node
    +init = (self, cond, body, otherwise=null):
        self.origin = null
        self.cond = cond
        self.body = body
        self.otherwise = otherwise

class InlineCond extends Node
    +init = (self, cond, body, otherwise):
        self.origin = null
        self.cond = cond
        self.body = body
        self.otherwise = otherwise

class Switch extends Node
    +init = (self, cond, body):
        self.origin = null
        self.cond = cond
        self.body = body

class While extends Node
    +init = (self, cond, body):
        self.origin = null
        self.cond = cond
        self.body = body

class DoWhile extends Node
    +init = (self, body, cond):
        self.origin = null
        self.body = body
        self.cond = cond

class For extends Node
    +init = (self, start, stop, step, body):
        self.origin = null
        self.start = start
        self.stop = stop
        self.step = step
        self.body = body

class Goto extends Node
    +init = (self, where):
        self.origin = null
        self.where = where

class Break extends Node
    +init = (self):
        self.origin = null
        null

class Continue extends Node
    +init = (self):
        self.origin = null
        null

class Return extends Node
    +init = (self, value):
        self.origin = null
        self.value = value

class Cast extends Node
    +init = (self, type, expr):
        self.origin = null
        self.type = type
        self.expr = expr

class GetItem extends Node
    +init = (self, base, index):
        self.origin = null
        self.base = base
        self.index = index

class Call extends Node
    +init = (self, callee, args):
        self.origin = null
        self.callee = callee
        self.args = args

class GetAttr extends Node
    +init = (self, expr, name, pointer=false):
        self.origin = null
        self.expr = expr
        self.name = name
        self.pointer = pointer



# Standard macro environment. Well...
std_macro_env = ():
    return {
        # __DATE__, literal of the form "Mmm dd yyyy", TODO
        "__DATE__": macrodef_string("Mmm dd yyyy")
        # __STDC__ should be '1'
        "__STDC__": macrodef_number("1")
        # __STDC_HOSTED__ should be '1' if implementation is hosted implementation ?whateverthatmeans?
        #                 or '0' if it's not. Lets use '1' like gcc.
        "__STDC_HOSTED__": macrodef_number("1")
        # __STDC_MB_MIGHT_NEQ_WC__ not sure what this means..
        # "The integer constant 1, intended to indicate that, in
        # the encoding for wchar_t,a member of the basic character set need not
        # have a code value equal to its value when used as the lone character in an
        # integer character constant"
        "__STDC_HOSTED__": macrodef_number("199901L")
        "__TIME__": macrodef_string("hh:mm:ss")
        # __STDC_IEC_559__ The integer constant 1, intended to indicate conformance to the
        #                  specifications in annexF(IEC 60559 floating-point arithmetic)
        "__STDC_IEC_559__": macrodef_number("1")
        # __STDC_IEC_559_COMPLEX__ The integer constant 1, intended to indicate
        #                          adherence to the specifications in informative annexG(IEC 60559
        #                          compatible complex arithmetic).
        "__STDC_IEC_559_COMPLEX__": macrodef_number("1")
    }
    # umm.. not sure what this is.
    # __STDC_ISO_10646__ An integer constant of the form yyyymmL (for example,
    # 199712L). If this symbol is defined, then every character in the Unicode
    # required set, when stored in an object of type wchar_t, has the same
    # value as the short identifier of that character. The Unicode required set
    # consists of all the characters that are defined by ISO/IEC 10646, along with
    # all amendments and technical corrigenda, as of the specified year and
    # month
    # _Pragma(args) should apparently have same behavior as #pragma.. implementing this later if it appears?

macrodef_string = (string):
    return :exnihilo()
        origin = null
        parameters = null
        replacement = [ Token(path(""), 0, "string", string) ]

macrodef_number = (number):
    if isinstance(number, int)
        number = number.to_string()
    return :exnihilo()
        origin = null
        parameters = null
        replacement = [ Token(path(""), 0, "number", number) ]

class Preprocessor
    +init = (self, config, macro_replacements, stream):
        self.config = config
        # There is one name space for macro names, despite there are function
        # macros.
        self.macro_replacements = macro_replacements
        self.file_stack = [] # stack for #include.

        self.search_paths = self.include_search_paths(true)
        self.filename = stream.filename # we catch recursive includes with this.
        self.stream = stream
        self.depth = 0 # how deep conditional groups, tells whether #endif/#else accepted
        self.putback = []

        self.empty = false # make it easy to determine when the
                           # preprocessor stream is empty.

    next_token = (self, macroline):
        while self.putback.length > 0
            return self.putback.pop()
        return token_chop(self.stream, macroline)

    # After the arguments have been identified, arguments are substituted.
    # If parameter precedes '#' or '##', or is followed by '##', it is handled differently.
    # When parameter appears in a replacement list, it is replaced by the corresponding
    # argument after all macros contained in the argument are expanded.

    # What does this mean? "Before being substituted, each argument’s preprocessing tokens are
    #                       completely macro replaced as if they formed the rest of the preprocessing file; no other
    #                       preprocessing tokens are available"

    # if '#' appears in replacement list after parameter, the following
    # parameter is replaced by string literal that contains the spelling of the
    # token sequence for the argument.

    # if '#' and '##' are mixed, the behavior is unspecified. Maybe best idea to throw error here.

    # If, in the replacement list of a function-like macro, a parameter is immediately preceded
    # or followed by a ## preprocessing token, the parameter is replaced by the corresponding
    # argument’s preprocessing token sequence; however, if an argument consists of no
    # preprocessing tokens, the parameter is replaced by a placemarker preprocessing token
    # instead. -- not sure what this means..

    # Apparently the parameter list is immediately replaced and the '##' is re-examined.
    # The placemarker is just marking place so the '##' won't misbehave.
    # Because this thing is supposed to merge tokens.

    # Again if the merged token is not valid preprocessing token, the behavior can be undefined.

    # The resulting token is available for further macro replacement?! :o

    # Hm. I can use a simple tactic to do macro replacements then.
    macroexpand = (self, macroline=false):
        while true
            token = self.next_token(macroline)
            if not token or token.name != "identifier"
                return token
            # when inside the macro we should handle 'defined' separately.
            # defined identifier and defined ( identifier )
            # should be supported inside conditional exec. it should return '0' or '1'
            if macroline and token.value == "defined"
                token.name = "defined"
                env = self.capture(token, ["defined"], macroline)
                if env
                    defined = env["defined"]
                    assert defined.length == 1 and defined[0].name == "identifier"
                        format_source(token) ++ "'defined' with bad argument"
                    defined = defined[0]
                else
                    defined = self.next_token(macroline)
                    assert defined and defined.name == "identifier"
                        format_source(token) ++ "'defined' without identifier"
                if defined.value in self.macro_replacements
                    result = '1'
                else
                    result = '0'
                return :Token(token.filename, token.lno, "number", result)
                    macro = token.macro
            if token.value == "__FILE__" # dynamically defined macros.
                token.name = "string"
                token.value = self.stream.filename.to_string()
                return token
            elif token.value == "__LINE__"
                token.name = "number"
                token.value = self.stream.lno.to_string()
                return token
            elif token.value not in self.macro_replacements
                return token
            t = token.macro
            while t
                if t.value == token.value # This and Token.clone prevents
                    return token          # self-referential macros from expanding.
                t = t.macro
            subs = self.macro_replacements[token.value]
            # here if there are parameters, we should go and see if there's '('
            # somewhere ahead in the context.
            if subs.parameters
                env = self.capture(token, subs.parameters, macroline)
                if not env       # This macro function didn't get any
                    return token # parameters, so it cannot expand.
            else
                env = {}
            replacement = []
            for rtoken in subs.replacement
                t = rtoken.clone(token) # TODO: implement '#' and '##' here.
                assert t.value != '#', "implement #" ++ format_source(token)
                if t.value in env                    # here the arguments are
                    r = env[t.value]                 # substituted in.
                    if r.length > 0
                        replacement.extend(r) 
                    else
                        replacement.append(null)     # drop placeholder
                else
                    replacement.append(t)
            for i in range(replacement.length - 1, 0, -1)
                if replacement[i].value == '##'
                    xx = replacement.pop(i)
                    rhs = replacement.pop(i)
                    lhs = replacement[i-1].clone(xx) # the macro might duplicate a token.
                    lhs.value ++= rhs.value # TODO: should check that they are compatible tokens.
                    replacement[i-1] = lhs
            for t in reversed(replacement)
                if t # drop placeholder.
                    self.putback.append(t)

    # Once there's the macro expansion, it must
    # match parentheses until it hits a matching ')'
    # The macro expansion must be triggered by '('
    # if there's parameter list present.
    capture = (self, macro, parameters, macroline):
        token = self.next_token(macroline)
        if not token or token.value != '('
            self.putback.append(token)
            return null
        cap = []
        token = self.next_token(macroline)
        depth = 1
        if token and token.value == ')'
            depth -= 1
        while token and (depth > 0 or token.name == "macro")
            if token and token.value == '('
                depth += 1
            cap.append(token)
            token = self.next_token(macroline)
            if token and token.value == ')'
                depth -= 1
        assert depth == 0
            format_source(self.stream) ++ "unterminated argument list invoking macro " ++ repr(macro)
        env = {}
        args = 0
        for param in parameters
            # The identifier __VA_ARGS__ shall occur only in the replacement-list
            # of a function macro that ends with ellipsis notation.
            # The ellipsis cause remaining argument tokens to be swallowed, including commas,
            # into the last argument list.
            if param == "..."            # this is always as last.
                env["__VA_ARGS__"] = cap # we check it at #define
                cap = []
            elif args + 1 == parameters.length # last argument.
                depth = 0
                for t in cap
                    if t.value == '('
                        depth += 1
                    if t.value == ')'
                        depth -= 1
                    assert t.value != "," or depth > 0
                        format_source(self.stream) ++ "too many arguments passed to macro " ++ repr(macro)
                env[param] = cap
                cap = []
            else
                arg = []
                comma = false
                depth = 0
                while cap.length > 0
                    t = cap.pop(0)
                    if t.value == '('
                        depth += 1
                    if t.value == ')'
                        depth -= 1
                    if t.value == "," and depth == 0
                        comma = true
                        break
                    arg.append(t)
                assert comma and depth == 0
                    format_source(self.stream) ++ "too few arguments passed to macro"
                env[param] = arg
            args += 1
        return env

    skip_group = (self, skipper_macro):
        while true
            token = self.next_token(false)
            assert token
                format_source(skipper_macro) ++ "unterminated conditional group"
            if token.name != "macro"
                continue
            macro = token_chop(self.stream, true)
            if not macro # null directive handling.
                continue
            elif macro.value == "else"  # doing it this way allows user to
                self.macro_end(macro)   # use #else twice in a group. We may have to fix it later.
                self.depth += 1
                return null # #else/#elif in skipping causes resumption to normal execution.
            elif macro.value == "elif"            # TODO: should evaluate the expr here.
                macroseq = self.macro_macroexpand_list()
                if self.macroeval(macroseq)
                    self.depth += 1
                    return null # resumes to preprocessing.
            elif macro.value == "if"          # This requires some attention.
                self.skip_deep_group(macro)   # deep skipping here ensures nested
            elif macro.value == "ifdef"       # condition macros work as well.
                self.skip_deep_group(macro)
            elif macro.value == "ifndef"      # also used on #else/#elif after active #if
                self.skip_deep_group(macro)
            elif macro.value == "endif"
                return self.macro_end(macro)
    
    skip_deep_group = (self, skipper_macro):
        while true
            token = self.next_token(false)
            assert token
                format_source(skipper_macro) ++ "unterminated conditional group"
            if token.name != "macro"
                continue
            macro = token_chop(self.stream, true)
            if not macro # null directive handling.
                continue
            if macro.value == "if"          # note that we ignore #elif here.
                self.skip_deep_group(macro) # yet we go into deep group skip
            elif macro.value == "ifdef"     # when we see else.
                self.skip_deep_group(macro) # fix it? though maybe it doesn't
            elif macro.value == "ifndef"    # harm at this point.
                self.skip_deep_group(macro)
            elif macro.value == "endif"
                return self.macro_end(macro)

    step = (self):
        token = self.macroexpand()
        while not token and self.file_stack.length > 0
            assert self.depth == 0, "missing #endif at " ++ self.filename.to_string()
            entry = self.file_stack.pop()
            self.search_paths = entry.search_paths
            self.filename = entry.filename
            self.stream = entry.stream
            self.depth = entry.depth
            token = self.macroexpand()
        if not token
            assert self.depth == 0, "missing #endif"
            self.empty = true
            return null
        elif token.name != "macro"
            return token
        # Here the lines beginning with '#' are interpreted.
        assert self.putback.length == 0, "assumptions violated"
        macro = token_chop(self.stream, true)
        if not macro     # null directive '#' should not have effect.
            return null 
        assert macro.name == "identifier"
            format_source(self.stream) ++ "macro statement must start with identifier"

        # First here's support for #if and #elif, note that these groups can
        # be nested and they may trigger a skip.
        if macro.value == "if"
            macroseq = self.macro_macroexpand_list()
            if self.macroeval(macroseq)
                self.depth += 1
                return null
            else
                return self.skip_group(macro)
        elif macro.value == "ifdef"
            identifier = self.macro_identifier(macro)
            self.macro_end(macro)
            if identifier.value in self.macro_replacements
                self.depth += 1
                return null
            else
                return self.skip_group(macro)
        elif macro.value == "ifndef"
            identifier = self.macro_identifier(macro)
            self.macro_end(macro)
            if identifier.value not in self.macro_replacements
                self.depth += 1
                return null
            else
                return self.skip_group(macro)
        elif macro.value == "elif" and self.depth > 0 # here this means the block to evaluate ended. 
            self.depth -= 1 # the skip will consume #endif
            return self.skip_deep_group(macro)
        elif macro.value == "else" and self.depth > 0 # same thing as with #elif.
            self.depth -= 1
            return self.skip_deep_group(macro)
        elif macro.value == "endif" and self.depth > 0 # if the group was active
            self.depth -= 1                            # then the group above it was active too.
            return self.macro_end(macro)
        elif macro.value == "define"
            identifier = token_chop(self.stream, true)
            assert identifier and identifier.name == "identifier"
                format_source(macro) ++ "'#define' must start with identifier"
            parameters = null
            if self.stream.current == '('
                self.stream.advance()
                parameters = []
                token = token_chop(self.stream, true)
                while token and token.name != ')'
                    parameters.append(token.value)
                    assert token.name == 'identifier' or token.name == "..."
                        format_source(macro) ++ "malformed parameter list, expected identifier or '...'"
                    if token.value == "..."
                        token = token_chop(self.stream, true)
                        break
                    token = token_chop(self.stream, true)
                    if token.name == ")"
                        break
                    assert token and token.name == ","
                        format_source(macro) ++ "malformed parameter list, expected ','"
                    token = token_chop(self.stream, true)
                assert token and token.name == ')'
                    format_source(macro) ++
                    "'#define' parameter list must end with ')'"
            replacement = self.macro_noexpand_list()
            # The preprocessor should generate an error message from
            # a redefined macro whose replacement list is not identical
            # with the earlier list.
            assert identifier.value != "defined"
                format_source(identifier) ++ "defined cannot be used as a macro name"
            if identifier.value in self.macro_replacements
                mac = self.macro_replacements[identifier.value]
                assert mac.parameters == parameters and mac.replacement == replacement
                    print("old", identifier, mac.parameters, mac.replacement)
                    print("new", identifier, parameters, replacement)
                    format_source(macro) ++ "redefined macro not identical with already defined macro"
            else
                self.macro_replacements[identifier.value] = :exnihilo()
                    origin = identifier
                    parameters = parameters
                    replacement = replacement
            return null
        elif macro.value == "undef"
            identifier = self.macro_identifier(macro)
            if identifier.value in self.macro_replacements
                self.macro_replacements.pop(identifier.value)
            return self.macro_end(macro)
        # #include <> -- implementation defined too? :/
        #                search from global includes.
        # #include "" -- search is implementation defined
        #                gcc approach would be to look in the directory of the script, or then
        #                in the directories given with '-I'
        # check from these how it's done in GCC and imitate. The detail is important
        # although spec doesn't define it.
        elif macro.value == "include" or macro.value == "include_next"
            self.stream.skip_spaces()
            if self.stream.current == '<'
                self.stream.advance()
                string = ""
                while self.stream.current != '>' and self.stream.current != '\n' and self.stream.current != ''
                    string ++= self.stream.advance()
                assert self.stream.current == '>'
                    format_source(macro) ++ "#include < should end with >"
                self.stream.advance()
                local = false
            else
                string = token_chop(self.stream, true)
                assert string.name == "string"
                    format_source(macro) ++ "#include expects string or '<'"
                string = string.value
                local = true
            self.macro_end(macro)

            entry = :exnihilo()
                stream = self.stream
                search_paths = self.search_paths
                filename = self.filename
                depth = self.depth
            self.file_stack.append(entry)
            if macro.value == "include_next"
                entry.search_paths = list(entry.search_paths) # Allows the sequential include_next commands to succeed.
                self.search_paths = iter(entry.search_paths) # Makes sure the next search will trim out the paths it used.
            else
                self.search_paths = self.include_search_paths(local)
            self.stream = self.include(macro, string, self.search_paths)
            self.filename = self.stream.filename # to make sure the preprocessor keeps
            self.depth = 0                       # track of physical instead of logical filename.
            return null
        # need to implement #line digits newline-
        # need to implement #line filename digits newline-
        # also..  stuff inside #line can macroexpand

        # #pragma STDC -- no macroexpansion on directive
        # -- and you should provide FP_CONTRACT, FENV_ACCESS, CX_LIMITED_RANGE
        # -- otherwise implementation dependent

        # handle #pragma once?

        # diagnostic message #error produces sequence of preprocessing tokens.
        elif macro.value == "error"
            message = []
            # ?? Should this expand args?
            for token in self.macro_noexpand_list()
                message.append(stringify_token(token.name, token.value))
            assert false
                format_source(macro) ++ "#error: " ++ " ".join(message)
        else
            message = []
            for token in self.macro_noexpand_list()
                message.append(stringify_token(token.name, token.value))
            assert false
                format_source(macro) ++ " not implemented #" ++
                macro.value ++ " " ++ " ".join(macroseq or [])

    macro_identifier = (self, macro):
        token = self.next_token(true)
        assert token and token.name == "identifier"
            format_source(macro) ++ "macro expected identifier, got " ++
            stringify_token(token.name, token.value)
        return token

    macro_noexpand_list = (self):
        out = []
        token = self.next_token(true)
        while token
            out.append(token)
            token = self.next_token(true)
        return out

    macro_macroexpand_list = (self):
        out = []
        token = self.macroexpand(true)
        while token
            out.append(token)
            token = self.macroexpand(true)
        return out

    macro_end = (self, macro):
        assert not self.next_token(true)
            format_source(macro) ++ "macro contains excess garbage"

    include = (self, macro, string, search_paths):
        # 'local' ignored here for now.
        header = null
        for search_path in search_paths
            header = path(search_path) ++ string
            if fs.exists(header)
                break
            header = null
        assert header, format_source(macro) ++ "#include did not find this file"
        # Might make sense to allow this to be disabled.
        collision = 0
        for entry in self.file_stack
            collision += entry.filename == header
            assert collision < 2
                files = [header]
                for entry in self.file_stack
                    files.append(entry.filename)
                "recursive #include: " ++ repr(files)
        return open_token_stream(header)

    # We wouldn't have to lug search paths around without include_next
    # To allow include_next, it's convenient to create a generator.
    include_search_paths = (self, local):
        if local
            for search_path in self.config["local_includes"]
                yield search_path
        for search_path in self.config["includes"]
            yield search_path


    # nonprocessed tokens here translate into '0'
    macroeval = (self, macroseq):
        keywords = macroeval_language.table.keywords
        terminals = macroeval_language.table.terminals
        parser = macroeval_language.new_parser()
        for token in macroseq
            term = terminals[keywords.get(token.name, token.name)]
            assert parser.expecting(term)
                seq = []
                for m in macroseq
                    seq.append(repr(m))
                print("in eval expr", " ".join(seq))
                print(parser.expect...)
                print(term, token)
                print(token.filename, token.lno)
                "macroeval failed"
                #raise chartparser.SyntaxErrorExpected(parser.expect, token, source)
            parser.step(term, token, token, token)
        assert parser.accepted
            seq = []
            for m in macroseq
                seq.append(repr(m))
            print("in eval expr", " ".join(seq))
            print(parser.expect...) # TODO: improve the messaging
            "macroeval accept failed"
            # same here
            #raise chartparser.SyntaxErrorExpected(parser.expect, token.stop, source, true)
        # TODO: parse and evaluate the macro sequence.
        actions = :exnihilo()
            identifier = (token):
                return 0
            number = (token):
                return parse_number(token.value)
            char = (token):
                return ord(token.value)
            unary_not = (expr):
                return int(expr == 0)
            logical_and = (a, b):
                return int(a != 0 and b != 0)
            logical_or = (a, b):
                return int(a != 0 or b != 0)
            eq = (a, b):
                return int(a == b)
            ne = (a, b):
                return int(a != b)
            lt = (a, b):
                return int(a < b)
            gt = (a, b):
                return int(a > b)
            le = (a, b):
                return int(a <= b)
            ge = (a, b):
                return int(a >= b)
            add = (a, b):
                return a + b
            sub = (a, b):
                return a - b
            mul = (a, b):
                return a * b
            div = (a, b):
                return int(a / b)
            mod = (a, b):
                return a % b
            cond = (a, t, f):
                if a != 0
                    return t
                else
                    return f
        post = (rule, args, start, stop):
            return rule.annotation(actions, args, [])
        result = parser.traverse(post, null)

        ## use if you get a problem with macro evaluator.
        #seq = []
        #for m in macroseq
        #    seq.append(repr(m))
        #print("eval", " ".join(seq))
        #print("res =", result)
        return result != 0

macroeval_language = grammar_language.read_file(dir ++ "c-macroeval.grammar")
macroeval_language.new_parser = chartparser.preprocess(
    macroeval_language.grammar,
    macroeval_language.table.nonterminal("expr"))


open_token_stream = (filename):    
    filename = path(filename)
    source = fs.read_file(filename)
    getch = string_getch(source)
    getch = trigraph_getch(getch) # Assume we run with -trigraphs, (for fun)
    return LogicalCharacterStream(getch, 1, filename)

# works on both tokens and streams because HONK! HONK!
format_source = (token):
    return token.filename.to_string() ++ ":" ++ token.lno.to_string() ++ ": "

# This will not properly stringify macros, but they are easy to handle
# manually if it is ever needed.
stringify_token = (name, value):
    if name == 'string'
        return escape_string(value)
    elif name == 'char'
        return escape_string(value, "'")
    else
        return value

# hilariously, this will probably also work for Lever itself just fine.
# and perhaps I should provide it is as a library function.
# Though this should still leave here, because Lever strings aren't necessarily always C strings
escape_string = (string, terminal='"', esc=escape_sequences):
    out = [terminal]
    for ch in string
        n = ord(ch)
        if 0x20 <= n and n <= 0x7E or 0xFF < n  # who does remember this range? :)
            if ch == '\\'
                ch = '\\\\'
            elif ch == terminal
                ch = '\\' ++ ch
        else
            # Note this only works on unprintable ascii character range.
            a = "0123456789abcdef"[n >> 4]
            b = "0123456789abcdef"[n & 15]
            ch = '\\x' ++ a ++ b
            for item in esc.items()
                if item[1] == n
                    ch = '\\' ++ item[0]
        out.append(ch)
    out.append(terminal)
    return "".join(out)

# The tokenizer has been copied from pytci and then updated to make more sense.
# The preprocessor tokenizer, imitating the behavior of a popular C compiler.
token_chop = (stream, macroline=false):
    stream.skip_spaces()
    line_begin = false
    while stream.current == '\n'
        if macroline    # Macro should not go over newline.
            return null # Here we make that certain.
        stream.advance()
        stream.skip_spaces()
        # Tokenizer must mark "#" as MACRO, if it
        # appears in the beginning of a "logical" line
        # If you follow where this is going to, you will see it will also
        # accept %: as macro token, because it is translated to "#".
        # But then ## and %:%: aren't accepted.
        line_begin = true
    if stream.current == ""
        return null
    lno = stream.lno

    flavor = null
    # String literal prefix hack.
    if (stream.current in ["L", "U"] and stream.next_current == '"') or
       (stream.current == "L"        and stream.next_current == "'")
        flavor = stream.advance()
    # Identifier: any sequence of letters, digits, or underscores,
    #             which begins with a letter or underscore
    #             you may have to accept $ as a letter
    if stream.current.is_alpha() or stream.current == '_'
        value = stream.advance()
        while stream.current.is_alpha() or stream.current.is_digit() or stream.current == '_'
            value ++= stream.advance()
        return Token(stream.filename, lno, "identifier", value)
    # String literals start with: " ", ' ' literals cannot cross lines.
    # there is no way to escape backslash in #include <...>
    if stream.current in ["'", '"']
        stream.comments = false
        terminal = stream.advance()
        string = ""
        while stream.current != terminal
            assert stream.current != "", "unterminated string"
            assert stream.current != "\n", "unterminated string"
            character = stream.advance()
            if character == '\\'
                string ++= escape_sequence(stream)
            else
                string ++= character
        stream.comments = true
        terminal = stream.advance()
        if terminal == "'"
            return Token(stream.filename, lno, "char", string, flavor)
        elif terminal == '"'
            return Token(stream.filename, lno, "string", string, flavor)
        else
            assert false, "error in tokenizing"
    # Preprocessing number: Formally, preprocessing numbers begin
    #                       with an optional period, a required
    #                       decimal digit, and then continue with
    #                       any sequence of letters, digits, underscores,
    #                       periods, and exponents. Exponents are the
    #                       two-character sequences 
    character = stream.advance()
    if character.is_digit() or (character == "." and stream.current.is_digit())
        number = character
        while stream.current.is_alpha() or stream.current.is_digit() or stream.current in ['.', '_']
            character = stream.advance()
            number ++= character
            if character ++ stream.current in exponents
                number ++= stream.advance()
        return Token(stream.filename, lno, "number", number)
    # Comments. Defined such that comments are replaced with spaces.
    # This works as well. Though in GCC you can disable single line comments.
    # Once there's need, we may do the same. Flag for the character stream should take care of it well.
    if character == "/" and stream.current == "/"
        while stream.current != "\n" and stream.current != ""
            stream.advance()
        return token_chop(stream, macroline) # macroline still works here.
    if character == "/" and stream.current == "*"
        stream.advance()
        ch = stream.advance()
        while ch != ""
            if ch == "*" and stream.current == "/"
                stream.advance()
                return token_chop(stream, macroline)
            ch = stream.advance()
        assert false, "unterminated comment, an error?"
    # Valid punctuation characters are listed lower below.
    if character in punctuators
        punc = character
        pair = punc ++ stream.current
        while pair in long_punctuators
            punc ++= stream.advance()
            pair = punc ++ stream.current
        if pair in digraphs
            punc = digraphs[punc ++ stream.advance()]
            if pair == "%:" and stream.current == "%" and stream.next_current == ":"
                stream.advance() # Hack to handle 4-character digraphs.
                stream.advance() # Only reason to look ahead twice.
                punc = "##"
            # Bit of C-trivia. GCC doesn't translate %:# to ##. Neither does this!
        if punc == '#' and line_begin
            return Token(stream.filename, stream.lno, "macro", punc)
        return Token(stream.filename, lno, punc, punc) # punctuation is named for parsing.
    return Token(stream.filename, lno, "other", character)

escape_sequence = (stream):
    if stream.current in escape_sequences
        return stream.advance()
    string = stream.advance()
    #\xhh The character whose numerical value is given by hh interpreted as a hexadecimal number
    if string == 'x'
        code = get_hex(stream) ++ get_hex(stream)
        if code.length == 2
            return chr(parse_int(code, 16))
        return "\\" ++ string ++ code
    #\nnn The character whose numerical value is given by nnn interpreted as an octal number
    if is_octal_char(string)
        string ++= get_octal(stream) ++ get_octal(stream)
        #if string.length == 3 # removed because all octal sequences are accepted.
        return chr(parse_int(string, 8))
    return "\\" ++ string

get_hex = (stream):
    if stream.current in hex_alphabet
        return stream.advance()
    return ""

get_octal = (stream):
    if is_octal_char(stream.current)
        return stream.advance()
    return ""

is_octal_char = (character):
    return character in octal_alphabet

class Token
    +init = (self, filename, lno, name, value="", flavor=""):
        self.filename = filename
        self.lno = lno
        self.name = name
        self.value = value
        self.macro = null # If this token came by macro substitution, this is set.
                          # Reveals self-referential tokens, as well as
                          # provides debugging information.
        self.flavor = flavor

    # Even if stringify_token is defined above, this piece
    # is not significant enough dependency that this class
    # should be moved higher.
    +repr = (self):
        return stringify_token(self.name, self.value)

    clone = (self, macro):
        return :Token(self.filename, self.lno, self.name, self.value)
            macro = macro
    
# Two replacement lists are identical if, and only if the preprocessing
# tokens in both have the same number, ordering, spelling and white-space
# separation, where all white-space separations are considered identical.

# This is meant for that comparison.
# Implementing this method allows us to compare two token lists directly.
%"=="[[Token, Token]] = (a, b):
    return a.name == b.name and a.value == b.value
    


hex_alphabet = set("0123456789ABCDEFabcdef")

octal_alphabet = set("01234567")

escape_sequences = {"a": 0x07, "b": 0x08, "f": 0x0C, "n": 0x0A, "r": 0x0D, "t": 0x09, "v": 0x0B, "\\": 0x5C, "'": 0x27, "\"": 0x22, "?": 0x3F}

exponents = set(["e+", "e-", "E+", "E-", "p+", "p-", "P+", "P-"])

punctuators = set([
    "!", "#", "$", "%", "&", "(", ")", "*", "+", 
    ",", "-", ".", "/", ":", ";", "<", "=", ">", "?", "[", 
    "\\", "]", "^", "_", "{", "|", "}", "~",
])

long_punctuators = set([
    "<=", ">=", "!=", "&&", "||", "++", "--", "==", "<<", ">>", "+=",
    "-=", "*=", "/=", "%=", "&=", "^=", "|=", "->", "..", "##",
    "...", "<<=", ">>="
])

digraphs = {"<%":"{", "&>":"}", "<:":"[", ":>":"]", "%:":"#", "%:%:":"##"}

# The stream of characters that come in from a file is the physical stream.
# We get logical character stream by translating trigraphs and joining lines
# ending with backslash.

# The tokenizer needs to look ahead two characters, but because C has escape
# for end of line, this will need some more character lookaheads to translate
# the physical lines into logical lines.
class LogicalCharacterStream
    +init = (self, getch, lno=1, filename=""):
        self.getch = getch
        self.lno = lno
        self.filename = filename
        self.current = '\n' # Begins with newline to get a subsequent hash
                            # character recognized as a macro token.
        self.ch1 = self.getch()
        self.ch2 = self.getch()
        self.ch3 = self.getch()

    # It's so simple to keep track of line numbers when you transform the
    # logical newline at the last possible moment.
    next_current = :property()
        get = (self):
            if self.ch1 == '\\' and self.ch2 == '\n'
                return self.ch3
            return self.ch1

    advance = (self):
        ch = self.current
        self.current = self.ch1
        self.ch1 = self.ch2
        self.ch2 = self.ch3
        self.ch3 = self.getch()
        if self.current == '\n'
            self.lno += 1
        # Here the physical lines turn into logical ones.
        while self.current == '\\' and self.ch1 == '\n'
            self.lno += 1             # Tracking by physical line numbers
            self.current = self.ch2   # is bit more difficult than it sounds
            self.ch1 = self.ch3       # at first.
            self.ch2 = self.getch()
            self.ch3 = self.getch()
        return ch

    is_space = (self):                  # Matters for 'define' macro after tokenizing.
        return self.current in spaces

    skip_spaces = (self):               # The tokenizer uses this twice.
        while self.current in spaces
            self.advance()

    skip_spaces_and_newlines = (self):  # Though I have forgotten why this was here...
        while self.current in spaces_and_newlines # And twice use of the above function
            self.advance()              # is also bit low to reason a break into function.

spaces = set('\x00 \t\f')
spaces_and_newlines = set('\x00 \t\f\n')

# The one of the most beloved features of C/C++.
# https://en.wikipedia.org/wiki/Digraphs_and_trigraphs#C
#
# When trigraphs are used this happens before the logical character stream.
trigraph_getch = (getch):
    ch0 = getch()
    ch1 = getch()
    return ():
        ch2 = getch()
        if ch0 == '?' and ch1 == '?' # This is equivalent to how big compilers
            try                      # are doing it.
                ch = trigraphs[ch2]  # Three-character window and checking if
                ch0 := getch()       # There's a trigraph on it.
                ch1 := getch()
                return ch
            except KeyError as _
                null
        ch = ch0
        ch0 := ch1
        ch1 := ch2
        return ch

trigraphs = {
    "=": "#",
    "/": "\\",
    "(": "[",
    ")": "]",
    "!": "|",
    "<": "{",
    ">": "}",
    "-": "~",
}

# The getch function works better for a tokenizer than an iterator, so
# we have this function to transform a string into a getch function.
string_getch = (string):
    gen = iter(string)
    return ():
        try
            ch = gen.next()
            if ch == '\r' # support CRLF, just in case some retards use them.
                return gen.next()
            return ch
        except UncatchedStopIteration as stop
            return "" # Some things are simpler when everything
                      # is a string. Therefore this is not a null.

parse_number = (number):
    if number.startswith("0x") or number.startswith("0X")
        return parse_int(number[2 .:], 16)
    if number.startswith("0")
        return parse_int(number, 8)
    return parse_int(number)
