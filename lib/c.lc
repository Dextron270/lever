# Completion of this strategy is due to 0.9.0 milestone
# STRATEGY: Improve foreign function interface to extend it's capabilities.
#           Provide complete REPL+Integrated Editor suite for lever.
# 
# I want people to frown when they see lever FFI and then contempt their
# language do not have this kind of system. To do this it needs to be
# slightly better and get some more exposure.

# Powerful interactive programming is lever's staple feature. This means we
# need really powerfull integrated development environment embedded into the
# runtime. The first step would be to provide a REPL that does bit more than
# reading a line and printing another out.

# TACTIC: use readline via FFI
# motivation: 
#   Line-editing tools in console are a great way to prepare
#   for the needs of a real IDE early on. It is also a feature many
#   other dynamic language implementations lack.
#
# requirements:
#   Use of readline requires that callbacks are implemented,
#   It also requires that library variables are handled more like
#   ordinary memory entries, because readline relies on global variables.

#   Use of readline requires dynamic loading that interrupts if the
#   library isn't available.

#   Also we may want to use this feature async. Async readline that doesn't
#   get confused by printing lines out would be so
#   awesome that it gives a new meaning for awesome.

#   Use of readline requires good C header generation. The cffi-gen library
#   is clumsy to use and update. Partially my own fault because I relied on
#   LR parsing. 

#   I believe it'd be the time to implement cffi header generator in lever.

#   To do that we need a C parser. Wholly functioning C parser! With macro
#   preprocessor.

#   This motivates the storyline of c.lc

#   This library should implement C tokenizing and parsing.
#   The parts that are common when you need to this kind of things.

import fs, json

main = ():
    stdio_h = "/usr/include/stdio.h"
    source = fs.read_file(stdio_h)
    getch = string_getch(source)
    getch = trigraph_getch(getch) # Assume we run with -trigraphs, (for fun)

    # There is one name space for macro names, despite there are function
    # macros.
    macro_replacements = {}
    # predefined macros
    # __DATE__, literal of the form "Mmm dd yyyy"
    # __FILE__, __LINE__
    # __STDC__ should be '1'
    # __STDC_HOSTED__ should be '1' if implementation is hosted implementation ?whateverthatmeans?
    #                 or '0' if it's not. Lets use '1' like gcc.
    # __STDC_MB_MIGHT_NEQ_WC__ not sure what this means..
    # "The integer constant 1, intended to indicate that, in
    # the encoding for wchar_t,a member of the basic character set need not
    # have a code value equal to its value when used as the lone character in an
    # integer character constant"

    # __STDC_VERSION__ The integer constant 199901L
    # __TIME__ The time of translation of the preprocessing translation unit. "hh:mm:ss"

    # __STDC_IEC_559__ The integer constant 1, intended to indicate conformance to the
    #                  specifications in annexF(IEC 60559 floating-point arithmetic)

    # __STDC_IEC_559_COMPLEX__ The integer constant 1, intended to indicate
    #                          adherence to the specifications in informative annexG(IEC 60559
    #                          compatible complex arithmetic).

    # umm.. not sure what this is.
    # __STDC_ISO_10646__ An integer constant of the form yyyymmL (for example,
    # 199712L). If this symbol is defined, then every character in the Unicode
    # required set, when stored in an object of type wchar_t, has the same
    # value as the short identifier of that character. The Unicode required set
    # consists of all the characters that are defined by ISO/IEC 10646, along with
    # all amendments and technical corrigenda, as of the specified year and
    # month
    
    # _Pragma(args) should apparently have same behavior as #pragma.. implementing this later if it appears?

    #ch = getch()
    #while ch != ''
    #    print(ch) # yaaay!!!
    #    ch = getch()
    stream = LogicalCharacterStream(getch, 1, stdio_h)

    # Once there's the macro expansion, it must
    # match parentheses until it hits a matching ')'
    # The macro expansion must be triggered by '('
    # if there's parameter list present.

    # The identifier __VA_ARGS__ shall occur only in the replacement-list
    # of a function macro that ends with ellipsis notation.
    # The ellipsis cause remaining argument tokens to be swallowed, including commas,
    # into the last argument list.

    # After the arguments have been identified, arguments are substituted.
    # If parameter precedes '#' or '##', or is followed by '##', it is handled differently.
    # When parameter appears in a replacement list, it is replaced by the corresponding
    # argument after all macros contained in the argument are expanded.

    # What does this mean? "Before being substituted, each argument’s preprocessing tokens are
    #                       completely macro replaced as if they formed the rest of the preprocessing file; no other
    #                       preprocessing tokens are available"

    # if '#' appears in replacement list after parameter, the following
    # parameter is replaced by string literal that contains the spelling of the
    # token sequence for the argument.

    # if '#' and '##' are mixed, the behavior is unspecified. Maybe best idea to throw error here.

    # If, in the replacement list of a function-like macro, a parameter is immediately preceded
    # or followed by a ## preprocessing token, the parameter is replaced by the corresponding
    # argument’s preprocessing token sequence; however, if an argument consists of no
    # preprocessing tokens, the parameter is replaced by a placemarker preprocessing token
    # instead. -- not sure what this means..

    # Apparently the parameter list is immediately replaced and the '##' is re-examined.
    # The placemarker is just marking place so the '##' won't misbehave.
    # Because this thing is supposed to merge tokens.

    # Again if the merged token is not valid preprocessing token, the behavior can be undefined.

    # The resulting token is available for further macro replacement?! :o

    # I can use a following tactic here:
    #   form a stack from the expanded macros, such that
    #   the context is aware of the word considered for
    #   expansion.

    #   if a token is a valid macro word, push replacement
    #   into the stack.

    #   if the token is valid macro function, push it into
    #   a stack and wait for '('.
    
    #   this lets you to suppress recursive macros.

    #   if you got a macro function pending and the next token is not '(', flush
    #   the macro token.


    # Printing tokens out is easy way to see that the stream is correctly tokenized.
    filename = stdio_h
    lno = 1
    unprinted = []
    while stream.current != ""
        token = token_chop(stream)
        if not token
            break
        # Here the lines beginning with '#' are interpreted.
        if token.name == "macro"
            macro = token_chop(stream, true)
            assert macro and macro.name == "identifier"
                format_source(stream) ++ "macro statement must start with identifier"
            if macro.value == "define"
                identifier = token_chop(stream, true)
                assert identifier and identifier.name == "identifier"
                    format_source(macro) ++ "'#define' must start with identifier"
                parameters = null
                if stream.current == '('
                    stream.advance()
                    parameters = []
                    token = token_chop(stream, true)
                    while token and token.name != ')'
                        parameters.append(token.value)
                        assert token.name == 'identifier' or token.name == "..."
                            format_source(macro) ++ "malformed parameter list, expected identifier or '...'"
                        if token.value == "..."
                            token = token_chop(stream, true)
                            break
                        token = token_chop(stream, true)
                        if token.name == ")"
                            break
                        assert token and token.name == ","
                            format_source(macro) ++ "malformed parameter list, expected ','"
                        token = token_chop(stream, true)
                    assert token and token.name == ')'
                        format_source(macro) ++
                        "'#define' parameter list must end with ')'"
                replacement = []
                token = token_chop(stream, true)
                while token
                    replacement.append(token)
                    token = token_chop(stream, true)
                print("DEFINE", identifier.value, parameters, replacement)
                # The preprocessor should generate an error message from
                # a redefined macro whose replacement list is not identical
                # with the earlier list.
                assert identifier.value != "defined"
                    format_source(identifier) ++ "defined cannot be used as a macro name"
                if identifier.value in macro_replacements
                    mac = macro_replacements[identifier.value]
                    assert mac.parameters == parameters and mac.replacement == replacement
                        format_source(macro) ++ "redefined macro not identical with already defined macro"
                else
                    macro_replacements[identifier.value] = :exnihilo()
                        parameters = parameters
                        replacement = replacement
                continue

            # you need to implement #undef

            # need to implement #line digits newline-
            # need to implement #line filename digits newline-
            # also..  stuff inside #line can macroexpand

            # diagnostic message #error produces sequence of preprocessing tokens.

            # #pragma STDC -- no macroexpansion on directive
            # -- and you should provide FP_CONTRACT, FENV_ACCESS, CX_LIMITED_RANGE
            # -- otherwise implementation dependent

            # handle #pragma once?

            # null directive '#' should not have effect.

            # defined identifier and defined ( identifier )
            # should be supported inside conditional exec. it should return '0' or '1'

            # nonprocessed tokens here translate into '0'

            # support #if, #elif, #else, #ifdef, #ifndef
            # note that these can be nested, and they may or may not trigger macros or
            # group being processed.
            
            # also, the macros of these things should expand inside them.

            # #include "" -- search is implementation defined
            #                gcc approach would be to look in the directory of the script, or then
            #                in the directories given with '-I'
            # #include <> -- implementation defined too? :/
            #                search from global includes.
            # check from these how it's done in GCC and imitate. The detail is important
            # although spec ignores it.

            macroseq = []
            token = token_chop(stream, true)
            while token
                macroseq.append(token)
                token = token_chop(stream, true)
            print("MACRO", macro.value, macroseq)
            continue

        # This thing here is trying to imitate the weird line switching logic
        # of gcc token printer. I think I get it wrong. But then I'm not yet
        # handling macros either.
        if token.lno != lno or token.filename != filename
            if token.filename == filename
                print(" ".join(unprinted))
                if lno < token.lno and token.lno < lno + 6
                    for i in range(lno, token.lno-1)
                        print("")
                else
                    print("")
                    print("#", token.lno, escape_string(filename))
            else
                print("")
                print("#", token.lno, escape_string(filename))
            filename = token.filename
            lno = token.lno
            unprinted = []
        unprinted.append(stringify_token(token.name, token.value))
    if unprinted.length > 0
        print(" ".join(unprinted))

    # This is actually something exposed to user. The user can select a config
    # The config comes from pytci's gcc_probe
    # Ran it with gcc_probe.py gcc -m32
    # and without the -m32
    #config = json.read_file("/home/cheery/.local/cc-config-x86_64.json")

    #for search_path in config["includes"]
    #    readline_h = path(search_path) ++ "readline/readline.h"
    #    if fs.exists(readline_h)
    #        print(readline_h)
    #    history_h = path(search_path) ++ "readline/history.h"
    #    if fs.exists(history_h)
    #        print(history_h)

# works on both tokens and streams because HONK! HONK!
format_source = (token):
    return token.filename ++ ":" ++ token.lno.to_string() ++ " "

# This will not properly stringify macros, but they are easy to handle
# manually if it is ever needed.
stringify_token = (name, value):
    if name == 'string'
        return escape_string(value)
    elif name == 'char'
        return escape_string(value, "'")
    else
        return value

# hilariously, this will probably also work for Lever itself just fine.
# and perhaps I should provide it is as a library function.
escape_string = (string, terminal='"', esc=escape_sequences):
    out = [terminal]
    for ch in string
        n = ord(ch)
        if 0x20 <= n and n <= 0x7E or 0xFF < n  # who does remember this range? :)
            if ch == terminal
                ch = '\\' ++ ch
        else
            # Note this only works on unprintable ascii character range.
            a = "0123456789abcdef"[n >> 4]
            b = "0123456789abcdef"[n & 15]
            ch = '\\x' ++ a ++ b
            for item in esc.items()
                if item[1] == n
                    ch = '\\' ++ item[0]
        out.append(ch)
    out.append(terminal)
    return "".join(out)

# The tokenizer has been copied from pytci and then updated to make more sense.
# The preprocessor tokenizer, imitating the behavior of a popular C compiler.
token_chop = (stream, macroline=false):
    stream.skip_spaces()
    line_begin = false
    while stream.current == '\n'
        if macroline    # Macro should not go over newline.
            return null # Here we make that certain.
        stream.advance()
        stream.skip_spaces()
        # Tokenizer must mark "#" as MACRO, if it
        # appears in the beginning of a "logical" line
        # If you follow where this is going to, you will see it will also
        # accept %: as macro token, because it is translated to "#".
        # But then ## and %:%: aren't accepted.
        line_begin = true
    if stream.current == ""
        return null
    lno = stream.lno
    # Identifier: any sequence of letters, digits, or underscores,
    #             which begins with a letter or underscore
    #             you may have to accept $ as a letter
    if stream.current.is_alpha() or stream.current == '_'
        value = stream.advance()
        while stream.current.is_alpha() or stream.current.is_digit() or stream.current == '_'
            value ++= stream.advance()
        return Token(stream.filename, lno, "identifier", value)
    # String literals start with: " ", ' ' literals cannot cross lines.
    # there is no way to escape backslash in #include <...>
    if stream.current in ["'", '"']
        stream.comments = false
        terminal = stream.advance()
        string = ""
        while stream.current != terminal
            assert stream.current != "", "unterminated string"
            assert stream.current != "\n", "unterminated string"
            character = stream.advance()
            if character == '\\'
                string ++= escape_sequence(stream)
            else
                string ++= character
        stream.comments = true
        terminal = stream.advance()
        if terminal == "'"
            return Token(stream.filename, lno, "char", string)
        elif terminal == '"'
            return Token(stream.filename, lno, "string", string)
        else
            assert false, "error in tokenizing"
    # Preprocessing number: Formally, preprocessing numbers begin
    #                       with an optional period, a required
    #                       decimal digit, and then continue with
    #                       any sequence of letters, digits, underscores,
    #                       periods, and exponents. Exponents are the
    #                       two-character sequences 
    character = stream.advance()
    if character.is_digit() or character == "." and stream.current.is_digit()
        number = character
        while stream.current.is_alpha() or stream.current.is_digit() or stream.current in ['.', '_']
            character = stream.advance()
            number ++= character
            if character ++ stream.current in exponents
                number ++= stream.advance()
        return Token(stream.filename, lno, "number", number)
    # Comments. Defined such that comments are replaced with spaces.
    # This works as well. Though in GCC you can disable single line comments.
    # Once there's need, we may do the same. Flag for the character stream should take care of it well.
    if character == "/" and stream.current == "/"
        while stream.current != "\n" and stream.current != ""
            stream.advance()
        return token_chop(stream, macroline) # macroline still works here.
    if character == "/" and stream.current == "*"
        stream.advance()
        ch = stream.advance()
        while ch != ""
            ch = stream.advance()
            if ch == "*" and stream.current == "/"
                stream.advance()
                return token_chop(stream, macroline)
        assert false, "unterminated comment, an error?"
    # Valid punctuation characters are listed lower below.
    if character in punctuators
        punc = character
        pair = punc ++ stream.current
        while pair in long_punctuators
            punc ++= stream.advance()
            pair = punc ++ stream.current
        if pair in digraphs
            punc = digraphs[punc ++ stream.advance()]
            if pair == "%:" and stream.current == "%" and stream.next_current == ":"
                stream.advance() # Hack to handle 4-character digraphs.
                stream.advance() # Only reason to look ahead twice.
                punc = "##"
            # Bit of C-trivia. GCC doesn't translate %:# to ##. Neither does this!
        if punc == '#' and line_begin
            return Token(stream.filename, stream.lno, "macro", punc)
        return Token(stream.filename, lno, punc, punc) # punctuation is named for parsing.
    return Token(stream.filename, lno, "other", current)

escape_sequence = (stream):
    if stream.current in escape_sequences
        return stream.advance()
    string = stream.advance()
    #\xhh The character whose numerical value is given by hh interpreted as a hexadecimal number
    if string == 'x'
        code = get_hex(stream) ++ get_hex(stream)
        if code.length == 2
            return chr(parse_int(code, 16))
        return "\\" ++ string ++ code
    #\nnn The character whose numerical value is given by nnn interpreted as an octal number
    if is_octal_char(string)
        string ++= get_octal(stream) ++ get_octal(stream)
        if string.length == 3
            return chr(parse_int(string, 8))
    return "\\" ++ string

get_hex = (stream):
    if stream.current in hex_alphabet
        return stream.advance()
    return ""

get_octal = (stream):
    if is_octal_char(stream.current)
        return stream.advance()
    return ""

is_octal_char = (character):
    return character in octal_alphabet

class Token
    +init = (self, filename, lno, name, value=""):
        self.filename = filename
        self.lno = lno
        self.name = name
        self.value = value

    # Even if stringify_token is defined above, this piece
    # is not significant enough dependency that this class
    # should be moved higher.
    +repr = (self):
        return stringify_token(self.name, self.value)
    
# Two replacement lists are identical if, and only if the preprocessing
# tokens in both have the same number, ordering, spelling and white-space
# separation, where all white-space separations are considered identical.

# This is meant for that comparison.
# Implementing this method allows us to compare two token lists directly.
%"=="[[Token, Token]] = (a, b):
    return a.name == b.name and a.value == b.value
    


hex_alphabet = set("0123456789ABCDEFabcdef")

octal_alphabet = set("01234567")

escape_sequences = {"a": 0x07, "b": 0x08, "f": 0x0C, "n": 0x0A, "r": 0x0D, "t": 0x09, "v": 0x0B, "\\": 0x5C, "'": 0x27, "\"": 0x22, "?": 0x3F}

exponents = set(["e+", "e-", "E+", "E-", "p+", "p-", "P+", "P-"])

punctuators = set([
    "!", "#", "$", "%", "&", "(", ")", "*", "+", 
    ",", "-", ".", "/", ":", ";", "<", "=", ">", "?", "[", 
    "\\", "]", "^", "_", "{", "|", "}", "~",
])

long_punctuators = set([
    "<=", ">=", "!=", "&&", "||", "++", "--", "==", "<<", ">>", "+=",
    "-=", "*=", "/=", "%=", "&=", "^=", "|=", "->", "..", "##",
    "...", "<<=", ">>="
])

digraphs = {"<%":"{", "&>":"}", "<:":"[", ":>":"]", "%:":"#", "%:%:":"##"}

# The stream of characters that come in from a file is the physical stream.
# We get logical character stream by translating trigraphs and joining lines
# ending with backslash.

# The tokenizer needs to look ahead two characters, but because C has escape
# for end of line, this will need some more character lookaheads to translate
# the physical lines into logical lines.
class LogicalCharacterStream
    +init = (self, getch, lno=1, filename=""):
        self.getch = getch
        self.lno = lno
        self.filename = filename
        self.current = '\n' # Begins with newline to get a subsequent hash
                            # character recognized as a macro token.
        self.ch1 = self.getch()
        self.ch2 = self.getch()
        self.ch3 = self.getch()

    # It's so simple to keep track of line numbers when you transform the
    # logical newline at the last possible moment.
    next_current = :property()
        get = (self):
            if self.ch1 == '\\' and self.ch2 == '\n'
                return self.ch3
            return self.ch1

    advance = (self):
        ch = self.current
        self.current = self.ch1
        self.ch1 = self.ch2
        self.ch2 = self.ch3
        self.ch3 = self.getch()
        if self.current == '\n'
            self.lno += 1
        # Here the physical lines turn into logical ones.
        while self.current == '\\' and self.ch1 == '\n'
            self.lno += 1             # Tracking by physical line numbers
            self.current = self.ch2   # is bit more difficult than it sounds
            self.ch1 = self.ch3       # at first.
            self.ch2 = self.getch()
            self.ch3 = self.getch()
        return ch

    is_space = (self):                  # Matters for 'define' macro after tokenizing.
        return self.current in spaces

    skip_spaces = (self):               # The tokenizer uses this twice.
        while self.current in spaces
            self.advance()

    skip_spaces_and_newlines = (self):  # Though I have forgotten why this was here...
        while self.current in spaces_and_newlines # And twice use of the above function
            self.advance()              # is also bit low to reason a break into function.

spaces = set('\x00 \t')
spaces_and_newlines = set('\x00 \t\n')

# The one of the most beloved features of C/C++.
# https://en.wikipedia.org/wiki/Digraphs_and_trigraphs#C
#
# When trigraphs are used this happens before the logical character stream.
trigraph_getch = (getch):
    ch0 = getch()
    ch1 = getch()
    return ():
        ch2 = getch()
        if ch0 == '?' and ch1 == '?' # This is equivalent to how big compilers
            try                      # are doing it.
                ch = trigraphs[ch2]  # Three-character window and checking if
                ch0 := getch()       # There's a trigraph on it.
                ch1 := getch()
                return ch
            except KeyError as _
                null
        ch = ch0
        ch0 := ch1
        ch1 := ch2
        return ch

trigraphs = {
    "=": "#",
    "/": "\\",
    "(": "[",
    ")": "]",
    "!": "|",
    "<": "{",
    ">": "}",
    "-": "~",
}

# The getch function works better for a tokenizer than an iterator, so
# we have this function to transform a string into a getch function.
string_getch = (string):
    gen = iter(string)
    return ():
        try
            ch = gen.next()
            if ch == '\r' # support CRLF, just in case some retards use them.
                return gen.next()
            return ch
        except UncatchedStopIteration as stop
            return "" # Some things are simpler when everything
                      # is a string. Therefore this is not a null.
