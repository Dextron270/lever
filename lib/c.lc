# Completion of this strategy is due to 0.9.0 milestone
# STRATEGY: Improve foreign function interface to extend it's capabilities.
#           Provide complete REPL+Integrated Editor suite for lever.
# 
# I want people to frown when they see lever FFI and then contempt their
# language do not have this kind of system. To do this it needs to be
# slightly better and get some more exposure.

# Powerful interactive programming is lever's staple feature. This means we
# need really powerfull integrated development environment embedded into the
# runtime. The first step would be to provide a REPL that does bit more than
# reading a line and printing another out.

# TACTIC: use readline via FFI
# motivation: 
#   Line-editing tools in console are a great way to prepare
#   for the needs of a real IDE early on. It is also a feature many
#   other dynamic language implementations lack.
#
# requirements:
#   Use of readline requires that callbacks are implemented,
#   It also requires that library variables are handled more like
#   ordinary memory entries, because readline relies on global variables.

#   Use of readline requires dynamic loading that interrupts if the
#   library isn't available.

#   Also we may want to use this feature async. Async readline that doesn't
#   get confused by printing lines out would be so
#   awesome that it gives a new meaning for awesome.

#   Use of readline requires good C header generation. The cffi-gen library
#   is clumsy to use and update. Partially my own fault because I relied on
#   LR parsing. 

#   I believe it'd be the time to implement cffi header generator in lever.

#   To do that we need a C parser. Wholly functioning C parser! With macro
#   preprocessor.

#   This motivates the storyline of c.lc

#   This library should implement C tokenizing and parsing.
#   The parts that are common when you need to this kind of things.

import fs, json

main = ():
    stdio_h = "/usr/include/stdio.h"

    # There is one name space for macro names, despite there are function
    # macros.

    # predefined macros
    macro_replacements = {
        # __DATE__, literal of the form "Mmm dd yyyy", TODO
        "__DATE__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "string", "Mmm dd yyyy") ]
        # __STDC__ should be '1'
        "__STDC__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "number", "1") ]
        # __STDC_HOSTED__ should be '1' if implementation is hosted implementation ?whateverthatmeans?
        #                 or '0' if it's not. Lets use '1' like gcc.
        "__STDC_HOSTED__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "number", "1") ]
        # __STDC_MB_MIGHT_NEQ_WC__ not sure what this means..
        # "The integer constant 1, intended to indicate that, in
        # the encoding for wchar_t,a member of the basic character set need not
        # have a code value equal to its value when used as the lone character in an
        # integer character constant"
        "__STDC_HOSTED__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "number", "199901L") ]
        "__TIME__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "string", "hh:mm:ss") ]

        # __STDC_IEC_559__ The integer constant 1, intended to indicate conformance to the
        #                  specifications in annexF(IEC 60559 floating-point arithmetic)
        "__STDC_IEC_559__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "number", "1") ]
        # __STDC_IEC_559_COMPLEX__ The integer constant 1, intended to indicate
        #                          adherence to the specifications in informative annexG(IEC 60559
        #                          compatible complex arithmetic).
        "__STDC_IEC_559_COMPLEX__": :exnihilo()
            parameters = null
            replacement = [ Token(path(""), 0, "number", "1") ]
    }

    # umm.. not sure what this is.
    # __STDC_ISO_10646__ An integer constant of the form yyyymmL (for example,
    # 199712L). If this symbol is defined, then every character in the Unicode
    # required set, when stored in an object of type wchar_t, has the same
    # value as the short identifier of that character. The Unicode required set
    # consists of all the characters that are defined by ISO/IEC 10646, along with
    # all amendments and technical corrigenda, as of the specified year and
    # month
    
    # _Pragma(args) should apparently have same behavior as #pragma.. implementing this later if it appears?

    # This is actually something exposed to user. The user can select a config
    # The config comes from pytci's gcc_probe
    # Ran it with gcc_probe.py gcc -m32
    # and without the -m32
    config = json.read_file("/home/cheery/.local/cc-config-x86_64.json")

    # giving the preprocessor a stream allows you to stream
    # from elsewhere than files. Although this uses the include like usual.
    preprocessor = Preprocessor(config, macro_replacements, open_token_stream(stdio_h))

    # Printing tokens out is easy way to see that the stream is correctly tokenized.
    filename = stdio_h
    lno = 1
    unprinted = []
    while not preprocessor.empty
        token = preprocessor.step()
        if not token # preprocessor returns 'null' also 
            continue # when macro processing is happening.


        # This thing here is trying to imitate the weird line switching logic
        # of gcc token printer. I think I get it wrong. But then I'm not yet
        # handling macros either.
        if token.lno != lno or token.filename != filename
            if token.filename == filename
                print(" ".join(unprinted))
                if lno < token.lno and token.lno < lno + 6
                    for i in range(lno, token.lno-1)
                        print("")
                else
                    print("")
                    print("#", token.lno, escape_string(filename))
            else
                print("")
                print("#", token.lno, escape_string(filename))
            filename = token.filename
            lno = token.lno
            unprinted = []
        unprinted.append(stringify_token(token.name, token.value))
    if unprinted.length > 0
        print(" ".join(unprinted))

class Preprocessor
    +init = (self, config, macro_replacements, stream):
        self.config = config
        self.macro_replacements = macro_replacements
        self.file_stack = [] # stack for #include.

        self.filename = stream.filename # we catch recursive includes with this.
        self.stream = stream
        self.depth = 0 # how deep conditional groups, tells whether #endif/#else accepted
        self.putback = []

        self.empty = false # make it easy to determine when the
                           # preprocessor stream is empty.

    next_token = (self, macroline):
        while self.putback.length > 0
            return self.putback.pop()
        return token_chop(self.stream, macroline)

    # After the arguments have been identified, arguments are substituted.
    # If parameter precedes '#' or '##', or is followed by '##', it is handled differently.
    # When parameter appears in a replacement list, it is replaced by the corresponding
    # argument after all macros contained in the argument are expanded.

    # What does this mean? "Before being substituted, each argument’s preprocessing tokens are
    #                       completely macro replaced as if they formed the rest of the preprocessing file; no other
    #                       preprocessing tokens are available"

    # if '#' appears in replacement list after parameter, the following
    # parameter is replaced by string literal that contains the spelling of the
    # token sequence for the argument.

    # if '#' and '##' are mixed, the behavior is unspecified. Maybe best idea to throw error here.

    # If, in the replacement list of a function-like macro, a parameter is immediately preceded
    # or followed by a ## preprocessing token, the parameter is replaced by the corresponding
    # argument’s preprocessing token sequence; however, if an argument consists of no
    # preprocessing tokens, the parameter is replaced by a placemarker preprocessing token
    # instead. -- not sure what this means..

    # Apparently the parameter list is immediately replaced and the '##' is re-examined.
    # The placemarker is just marking place so the '##' won't misbehave.
    # Because this thing is supposed to merge tokens.

    # Again if the merged token is not valid preprocessing token, the behavior can be undefined.

    # The resulting token is available for further macro replacement?! :o

    # Hm. I can use a simple tactic to do macro replacements then.
    macroexpand = (self, macroline=false):
        while true
            token = self.next_token(macroline)
            if not token or token.name != "identifier"
                return token
            # when inside the macro we should handle 'defined' separately.
            # defined identifier and defined ( identifier )
            # should be supported inside conditional exec. it should return '0' or '1'
            if macroline and token.value == "defined"
                token.name = "defined"
                t = self.next_token(macroline)
                assert t
                    format_source(token) ++ "'defined' without identifier"
                if t.name != "identifier"
                    assert t.value == "("
                        format_source(token) ++ "'defined' without identifier"
                    t = self.next_token(macroline)
                    u = self.next_token(macroline)
                    assert t and t.name == "identifier"
                        format_source(token) ++ "'defined' without identifier"
                    assert u and u.value == ")"
                        format_source(token) ++ "'defined' without identifier"
                token.value = t.value
                return token
            if token.value == "__FILE__" # dynamically defined macros.
                token.name = "string"
                token.value = self.stream.filename.to_string()
                return token
            elif token.value == "__LINE__"
                token.name = "number"
                token.value = self.stream.lno.to_string()
                return token
            elif token.value not in self.macro_replacements
                return token
            t = token.macro
            while t
                if t.value == token.value # This and Token.clone prevents
                    return token          # self-referential macros from expanding.
                t = t.macro
            subs = self.macro_replacements[token.value]
            # here if there are parameters, we should go and see if there's '('
            # somewhere ahead in the context.
            if subs.parameters
                env = self.capture(subs.parameters, macroline)
                if not env       # This macro function didn't get any
                    return token # parameters, so it cannot expand.
            else
                env = {}
            replacement = []
            for rtoken in subs.replacement
                t = rtoken.clone(token) # TODO: implement '#' and '##' here.
                if t.value in env                    # here the arguments are
                    replacement.extend(env[t.value]) # substituted in.
                else
                    replacement.append(t)
            
            for t in reversed(replacement)
                self.putback.append(t)

    # Once there's the macro expansion, it must
    # match parentheses until it hits a matching ')'
    # The macro expansion must be triggered by '('
    # if there's parameter list present.
    capture = (self, parameters, macroline):
        token = self.next_token(macroline)
        if not token or token.value != '('
            self.putback.append(token)
            return null
        cap = []
        token = self.next_token(macroline)
        depth = 1
        while token and (depth > 0 or token.name == "macro")
            cap.append(token)
            token = self.next_token(macroline)
            if token and token.value == '('
                depth += 1
            if token and token.value == ')'
                depth -= 1
        assert depth == 0
            format_source(self.stream) ++ "unterminated argument list invoking macro"
        env = {}
        for param in parameters
            # The identifier __VA_ARGS__ shall occur only in the replacement-list
            # of a function macro that ends with ellipsis notation.
            # The ellipsis cause remaining argument tokens to be swallowed, including commas,
            # into the last argument list.
            if param == "..."            # this is always as last.
                env["__VA_ARGS__"] = cap # we check it at #define
                cap = []
            elif args.length + 1 == parameters.length # last argument.
                for t in cap
                    assert t.value != ","
                        format_source(self.stream) ++ "too many arguments passed to macro"
                env[param] = cap
                cap = []
            else
                arg = []
                comma = false
                while cap.length > 0
                    t = cap.pop(0)
                    if t.value == ","
                        comma = true
                        break
                    arg.append(t)
                assert comma
                    format_source(self.stream) ++ "too few arguments passed to macro"
                env[param] = arg
        return env

    skip_group = (self, skipper_macro):
        while true
            token = self.next_token(false)
            assert token
                format_source(skipper_macro) ++ "unterminated conditional group"
            if token.name != "macro"
                continue
            macro = token_chop(self.stream, true)
            if not macro # null directive handling.
                continue
            elif macro.value == "else"  # doing it this way allows user to
                self.macro_end(macro)   # use #else twice in a group. We may have to fix it later.
                self.depth += 1
                return null # #else/#elif in skipping causes resumption to normal execution.
            elif macro.value == "elif"            # TODO: should evaluate the expr here.
                macroseq = self.macro_macroexpand_list()
                if self.macroeval(macroseq)
                    self.depth += 1
                    return null # resumes to preprocessing.
            elif macro.value == "if"          # This requires some attention.
                self.skip_deep_group(macro)   # deep skipping here ensures nested
            elif macro.value == "ifdef"       # condition macros work as well.
                self.skip_deep_group(macro)
            elif macro.value == "ifndef"      # also used on #else/#elif after active #if
                self.skip_deep_group(macro)
            elif macro.value == "endif"
                return self.macro_end(macro)
    
    skip_deep_group = (self, skipper_macro):
        while true
            token = self.next_token(false)
            assert token
                format_source(skipper_macro) ++ "unterminated conditional group"
            if token.name != "macro"
                continue
            macro = token_chop(self.stream, true)
            if not macro # null directive handling.
                continue
            if macro.value == "if"          # note that we ignore #elif here.
                self.skip_deep_group(macro) # yet we go into deep group skip
            elif macro.value == "ifdef"     # when we see else.
                self.skip_deep_group(macro) # fix it? though maybe it doesn't
            elif macro.value == "ifndef"    # harm at this point.
                self.skip_deep_group(macro)
            elif macro.value == "endif"
                return self.macro_end(macro)

    step = (self):
        token = self.macroexpand()
        while not token and self.file_stack.length > 0
            assert self.depth == 0, "missing #endif"
            entry = self.file_stack.pop()
            self.filename = entry.filename
            self.stream = entry.stream
            self.depth = entry.depth
            token = self.macroexpand()
        if not token
            assert self.depth == 0, "missing #endif"
            self.empty = true
            return null
        elif token.name != "macro"
            return token
        # Here the lines beginning with '#' are interpreted.
        assert self.putback.length == 0, "assumptions violated"
        macro = token_chop(self.stream, true)
        if not macro     # null directive '#' should not have effect.
            return null 
        assert macro.name == "identifier"
            format_source(self.stream) ++ "macro statement must start with identifier"
        # First here's support for #if and #elif, note that these groups can
        # be nested and they may trigger a skip.
        if macro.value == "if"
            macroseq = self.macro_macroexpand_list()
            if self.macroeval(macroseq)
                self.depth += 1
                return null
            else
                return self.skip_group(macro)
        elif macro.value == "ifdef"
            identifier = self.macro_identifier(macro)
            self.macro_end(macro)
            if identifier.value in self.macro_replacements
                self.depth += 1
                return null
            else
                return self.skip_group(macro)
        elif macro.value == "ifndef"
            identifier = self.macro_identifier(macro)
            self.macro_end(macro)
            if identifier.value not in self.macro_replacements
                self.depth += 1
                return null
            else
                return self.skip_group(macro)
        elif macro.value == "elif" and self.depth > 0 # here this means the block to evaluate ended. 
            self.depth -= 1 # the skip will consume #endif
            return self.skip_deep_group(macro)
        elif macro.value == "else" and self.depth > 0 # same thing as with #elif.
            self.depth -= 1
            return self.skip_deep_group(macro)
        elif macro.value == "endif" and self.depth > 0 # if the group was active
            self.depth -= 1                            # then the group above it was active too.
            return self.macro_end(macro)
        elif macro.value == "define"
            identifier = token_chop(self.stream, true)
            assert identifier and identifier.name == "identifier"
                format_source(macro) ++ "'#define' must start with identifier"
            parameters = null
            if self.stream.current == '('
                self.stream.advance()
                parameters = []
                token = token_chop(self.stream, true)
                while token and token.name != ')'
                    parameters.append(token.value)
                    assert token.name == 'identifier' or token.name == "..."
                        format_source(macro) ++ "malformed parameter list, expected identifier or '...'"
                    if token.value == "..."
                        token = token_chop(self.stream, true)
                        break
                    token = token_chop(self.stream, true)
                    if token.name == ")"
                        break
                    assert token and token.name == ","
                        format_source(macro) ++ "malformed parameter list, expected ','"
                    token = token_chop(self.stream, true)
                assert token and token.name == ')'
                    format_source(macro) ++
                    "'#define' parameter list must end with ')'"
            replacement = self.macro_noexpand_list()
            # The preprocessor should generate an error message from
            # a redefined macro whose replacement list is not identical
            # with the earlier list.
            assert identifier.value != "defined"
                format_source(identifier) ++ "defined cannot be used as a macro name"
            if identifier.value in self.macro_replacements
                mac = self.macro_replacements[identifier.value]
                assert mac.parameters == parameters and mac.replacement == replacement
                    format_source(macro) ++ "redefined macro not identical with already defined macro"
            else
                self.macro_replacements[identifier.value] = :exnihilo()
                    parameters = parameters
                    replacement = replacement
            return null
        elif macro.value == "undef"
            identifier = self.macro_identifier(macro)
            if identifier.value in self.macro_replacements
                self.macro_replacements.pop(identifier.value)
            return self.macro_end(macro)
        # #include <> -- implementation defined too? :/
        #                search from global includes.
        # #include "" -- search is implementation defined
        #                gcc approach would be to look in the directory of the script, or then
        #                in the directories given with '-I'
        # check from these how it's done in GCC and imitate. The detail is important
        # although spec doesn't define it.
        elif macro.value == "include"
            self.stream.skip_spaces()
            if self.stream.current == '<'
                self.stream.advance()
                string = ""
                while self.stream.current != '>' and self.stream.current != '\n' and self.stream.current != ''
                    string ++= self.stream.advance()
                assert self.stream.current == '>'
                    format_source(macro) ++ "#include < should end with >"
                self.stream.advance()
                local = false
            else
                string = token_chop(self.stream, true)
                assert string.name == "string"
                    format_source(macro) ++ "#include expects string or '<'"
                string = string.value
                local = true
            self.macro_end(macro)

            entry = :exnihilo()
                stream = self.stream
                filename = self.filename
                depth = self.depth
            self.file_stack.append(entry)
            self.stream = self.include(string, local)
            self.filename = self.stream.filename # to make sure the preprocessor keeps
            self.depth = 0                       # track of physical instead of logical filename.
            return null
        # need to implement #line digits newline-
        # need to implement #line filename digits newline-
        # also..  stuff inside #line can macroexpand

        # #pragma STDC -- no macroexpansion on directive
        # -- and you should provide FP_CONTRACT, FENV_ACCESS, CX_LIMITED_RANGE
        # -- otherwise implementation dependent

        # handle #pragma once?

        # diagnostic message #error produces sequence of preprocessing tokens.
        elif macro.value == "error"
            message = []
            # ?? Should this expand args?
            for token in self.macro_noexpand_list()
                message.append(stringify_token(token.name, token.value))
            assert false
                format_source(macro) ++ "#error: " ++ " ".join(message)
        else
            message = []
            for token in self.macro_noexpand_list()
                message.append(stringify_token(token.name, token.value))
            assert false
                format_source(macro) ++ " not implemented #" ++
                macro.value ++ " " ++ " ".join(macroseq)

    macro_identifier = (self, macro):
        token = self.next_token(true)
        assert token and token.name == "identifier"
            format_source(macro) ++ "macro expected identifier, got " ++
            stringify_token(token.name, token.value)
        return token

    macro_noexpand_list = (self):
        out = []
        token = self.next_token(true)
        while token
            out.append(token)
            token = self.next_token(true)
        return out

    macro_macroexpand_list = (self):
        out = []
        token = self.macroexpand(true)
        while token
            out.append(token)
            token = self.macroexpand(true)
        return out

    macro_end = (self, macro):
        assert not self.next_token(true)
            format_source(macro) ++ "macro contains excess garbage"

    include = (self, string, local):
        # 'local' ignored here for now.
        header = null
        for search_path in self.config["includes"]
            header = path(search_path) ++ string
            if fs.exists(header)
                break
            header = null
        assert header, format_source(macro) ++ "#include did not find this file"
        # Might make sense to allow this to be disabled.
        for entry in self.file_stack
            assert entry.filename != header, "recursive #include"
        return open_token_stream(header)

    # nonprocessed tokens here translate into '0'
    macroeval = (self, macroseq):
        # TODO: parse and evaluate the macro sequence.
        return true

open_token_stream = (filename):    
    filename = path(filename)
    source = fs.read_file(filename)
    getch = string_getch(source)
    getch = trigraph_getch(getch) # Assume we run with -trigraphs, (for fun)
    return LogicalCharacterStream(getch, 1, filename)

# works on both tokens and streams because HONK! HONK!
format_source = (token):
    return token.filename.to_string() ++ ":" ++ token.lno.to_string() ++ ": "

# This will not properly stringify macros, but they are easy to handle
# manually if it is ever needed.
stringify_token = (name, value):
    if name == 'string'
        return escape_string(value)
    elif name == 'char'
        return escape_string(value, "'")
    else
        return value

# hilariously, this will probably also work for Lever itself just fine.
# and perhaps I should provide it is as a library function.
escape_string = (string, terminal='"', esc=escape_sequences):
    out = [terminal]
    for ch in string
        n = ord(ch)
        if 0x20 <= n and n <= 0x7E or 0xFF < n  # who does remember this range? :)
            if ch == terminal
                ch = '\\' ++ ch
        else
            # Note this only works on unprintable ascii character range.
            a = "0123456789abcdef"[n >> 4]
            b = "0123456789abcdef"[n & 15]
            ch = '\\x' ++ a ++ b
            for item in esc.items()
                if item[1] == n
                    ch = '\\' ++ item[0]
        out.append(ch)
    out.append(terminal)
    return "".join(out)

# The tokenizer has been copied from pytci and then updated to make more sense.
# The preprocessor tokenizer, imitating the behavior of a popular C compiler.
token_chop = (stream, macroline=false):
    stream.skip_spaces()
    line_begin = false
    while stream.current == '\n'
        if macroline    # Macro should not go over newline.
            return null # Here we make that certain.
        stream.advance()
        stream.skip_spaces()
        # Tokenizer must mark "#" as MACRO, if it
        # appears in the beginning of a "logical" line
        # If you follow where this is going to, you will see it will also
        # accept %: as macro token, because it is translated to "#".
        # But then ## and %:%: aren't accepted.
        line_begin = true
    if stream.current == ""
        return null
    lno = stream.lno
    # Identifier: any sequence of letters, digits, or underscores,
    #             which begins with a letter or underscore
    #             you may have to accept $ as a letter
    if stream.current.is_alpha() or stream.current == '_'
        value = stream.advance()
        while stream.current.is_alpha() or stream.current.is_digit() or stream.current == '_'
            value ++= stream.advance()
        return Token(stream.filename, lno, "identifier", value)
    # String literals start with: " ", ' ' literals cannot cross lines.
    # there is no way to escape backslash in #include <...>
    if stream.current in ["'", '"']
        stream.comments = false
        terminal = stream.advance()
        string = ""
        while stream.current != terminal
            assert stream.current != "", "unterminated string"
            assert stream.current != "\n", "unterminated string"
            character = stream.advance()
            if character == '\\'
                string ++= escape_sequence(stream)
            else
                string ++= character
        stream.comments = true
        terminal = stream.advance()
        if terminal == "'"
            return Token(stream.filename, lno, "char", string)
        elif terminal == '"'
            return Token(stream.filename, lno, "string", string)
        else
            assert false, "error in tokenizing"
    # Preprocessing number: Formally, preprocessing numbers begin
    #                       with an optional period, a required
    #                       decimal digit, and then continue with
    #                       any sequence of letters, digits, underscores,
    #                       periods, and exponents. Exponents are the
    #                       two-character sequences 
    character = stream.advance()
    if character.is_digit() or character == "." and stream.current.is_digit()
        number = character
        while stream.current.is_alpha() or stream.current.is_digit() or stream.current in ['.', '_']
            character = stream.advance()
            number ++= character
            if character ++ stream.current in exponents
                number ++= stream.advance()
        return Token(stream.filename, lno, "number", number)
    # Comments. Defined such that comments are replaced with spaces.
    # This works as well. Though in GCC you can disable single line comments.
    # Once there's need, we may do the same. Flag for the character stream should take care of it well.
    if character == "/" and stream.current == "/"
        while stream.current != "\n" and stream.current != ""
            stream.advance()
        return token_chop(stream, macroline) # macroline still works here.
    if character == "/" and stream.current == "*"
        stream.advance()
        ch = stream.advance()
        while ch != ""
            ch = stream.advance()
            if ch == "*" and stream.current == "/"
                stream.advance()
                return token_chop(stream, macroline)
        assert false, "unterminated comment, an error?"
    # Valid punctuation characters are listed lower below.
    if character in punctuators
        punc = character
        pair = punc ++ stream.current
        while pair in long_punctuators
            punc ++= stream.advance()
            pair = punc ++ stream.current
        if pair in digraphs
            punc = digraphs[punc ++ stream.advance()]
            if pair == "%:" and stream.current == "%" and stream.next_current == ":"
                stream.advance() # Hack to handle 4-character digraphs.
                stream.advance() # Only reason to look ahead twice.
                punc = "##"
            # Bit of C-trivia. GCC doesn't translate %:# to ##. Neither does this!
        if punc == '#' and line_begin
            return Token(stream.filename, stream.lno, "macro", punc)
        return Token(stream.filename, lno, punc, punc) # punctuation is named for parsing.
    return Token(stream.filename, lno, "other", current)

escape_sequence = (stream):
    if stream.current in escape_sequences
        return stream.advance()
    string = stream.advance()
    #\xhh The character whose numerical value is given by hh interpreted as a hexadecimal number
    if string == 'x'
        code = get_hex(stream) ++ get_hex(stream)
        if code.length == 2
            return chr(parse_int(code, 16))
        return "\\" ++ string ++ code
    #\nnn The character whose numerical value is given by nnn interpreted as an octal number
    if is_octal_char(string)
        string ++= get_octal(stream) ++ get_octal(stream)
        if string.length == 3
            return chr(parse_int(string, 8))
    return "\\" ++ string

get_hex = (stream):
    if stream.current in hex_alphabet
        return stream.advance()
    return ""

get_octal = (stream):
    if is_octal_char(stream.current)
        return stream.advance()
    return ""

is_octal_char = (character):
    return character in octal_alphabet

class Token
    +init = (self, filename, lno, name, value=""):
        self.filename = filename
        self.lno = lno
        self.name = name
        self.value = value
        self.macro = null # If this token came by macro substitution, this is set.
                          # Reveals self-referential tokens, as well as
                          # provides debugging information.

    # Even if stringify_token is defined above, this piece
    # is not significant enough dependency that this class
    # should be moved higher.
    +repr = (self):
        return stringify_token(self.name, self.value)

    clone = (self, macro):
        return :Token(self.filename, self.lno, self.name, self.value)
            macro = macro
    
# Two replacement lists are identical if, and only if the preprocessing
# tokens in both have the same number, ordering, spelling and white-space
# separation, where all white-space separations are considered identical.

# This is meant for that comparison.
# Implementing this method allows us to compare two token lists directly.
%"=="[[Token, Token]] = (a, b):
    return a.name == b.name and a.value == b.value
    


hex_alphabet = set("0123456789ABCDEFabcdef")

octal_alphabet = set("01234567")

escape_sequences = {"a": 0x07, "b": 0x08, "f": 0x0C, "n": 0x0A, "r": 0x0D, "t": 0x09, "v": 0x0B, "\\": 0x5C, "'": 0x27, "\"": 0x22, "?": 0x3F}

exponents = set(["e+", "e-", "E+", "E-", "p+", "p-", "P+", "P-"])

punctuators = set([
    "!", "#", "$", "%", "&", "(", ")", "*", "+", 
    ",", "-", ".", "/", ":", ";", "<", "=", ">", "?", "[", 
    "\\", "]", "^", "_", "{", "|", "}", "~",
])

long_punctuators = set([
    "<=", ">=", "!=", "&&", "||", "++", "--", "==", "<<", ">>", "+=",
    "-=", "*=", "/=", "%=", "&=", "^=", "|=", "->", "..", "##",
    "...", "<<=", ">>="
])

digraphs = {"<%":"{", "&>":"}", "<:":"[", ":>":"]", "%:":"#", "%:%:":"##"}

# The stream of characters that come in from a file is the physical stream.
# We get logical character stream by translating trigraphs and joining lines
# ending with backslash.

# The tokenizer needs to look ahead two characters, but because C has escape
# for end of line, this will need some more character lookaheads to translate
# the physical lines into logical lines.
class LogicalCharacterStream
    +init = (self, getch, lno=1, filename=""):
        self.getch = getch
        self.lno = lno
        self.filename = filename
        self.current = '\n' # Begins with newline to get a subsequent hash
                            # character recognized as a macro token.
        self.ch1 = self.getch()
        self.ch2 = self.getch()
        self.ch3 = self.getch()

    # It's so simple to keep track of line numbers when you transform the
    # logical newline at the last possible moment.
    next_current = :property()
        get = (self):
            if self.ch1 == '\\' and self.ch2 == '\n'
                return self.ch3
            return self.ch1

    advance = (self):
        ch = self.current
        self.current = self.ch1
        self.ch1 = self.ch2
        self.ch2 = self.ch3
        self.ch3 = self.getch()
        if self.current == '\n'
            self.lno += 1
        # Here the physical lines turn into logical ones.
        while self.current == '\\' and self.ch1 == '\n'
            self.lno += 1             # Tracking by physical line numbers
            self.current = self.ch2   # is bit more difficult than it sounds
            self.ch1 = self.ch3       # at first.
            self.ch2 = self.getch()
            self.ch3 = self.getch()
        return ch

    is_space = (self):                  # Matters for 'define' macro after tokenizing.
        return self.current in spaces

    skip_spaces = (self):               # The tokenizer uses this twice.
        while self.current in spaces
            self.advance()

    skip_spaces_and_newlines = (self):  # Though I have forgotten why this was here...
        while self.current in spaces_and_newlines # And twice use of the above function
            self.advance()              # is also bit low to reason a break into function.

spaces = set('\x00 \t')
spaces_and_newlines = set('\x00 \t\n')

# The one of the most beloved features of C/C++.
# https://en.wikipedia.org/wiki/Digraphs_and_trigraphs#C
#
# When trigraphs are used this happens before the logical character stream.
trigraph_getch = (getch):
    ch0 = getch()
    ch1 = getch()
    return ():
        ch2 = getch()
        if ch0 == '?' and ch1 == '?' # This is equivalent to how big compilers
            try                      # are doing it.
                ch = trigraphs[ch2]  # Three-character window and checking if
                ch0 := getch()       # There's a trigraph on it.
                ch1 := getch()
                return ch
            except KeyError as _
                null
        ch = ch0
        ch0 := ch1
        ch1 := ch2
        return ch

trigraphs = {
    "=": "#",
    "/": "\\",
    "(": "[",
    ")": "]",
    "!": "|",
    "<": "{",
    ">": "}",
    "-": "~",
}

# The getch function works better for a tokenizer than an iterator, so
# we have this function to transform a string into a getch function.
string_getch = (string):
    gen = iter(string)
    return ():
        try
            ch = gen.next()
            if ch == '\r' # support CRLF, just in case some retards use them.
                return gen.next()
            return ch
        except UncatchedStopIteration as stop
            return "" # Some things are simpler when everything
                      # is a string. Therefore this is not a null.
